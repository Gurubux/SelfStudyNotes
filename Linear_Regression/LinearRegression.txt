LINEAR REGRESSION
******************************************************************************************************************************************************
STATISTICAL INFERENCE techniques including the t-test, chi-squared test and ANOVA which let you "ANALYZE DIFFERENCES BETWEEN DATA SAMPLES."
PREDICTIVE MODELING is a form of machine learning, which describes using computers to "AUTOMATE THE PROCESS OF FINDING PATTERNS" in data.
------------------------------------------------------
Linear Regression Basics
------------------------------------------------------
The term "regression" in predictive modeling generally refers to any modeling task that involves predicting a real number (as opposed classification, which involves predicting a category or class.).
The term "linear" in the name linear regression refers to the fact that the method models data with linear combination of the explanatory variables.
A linear combination is an expression where one or more variables are scaled by a constant factor and added together.
In the case of linear regression with a single explanatory variable, the linear combination used in linear regression can be expressed as:
	  ___________________________________________________	
 	 /	RESPONSE = INTERCEPT + CONSTANT ∗ EXPLANATORY 	 /
	/___________________________________________________/
	The right side of the equation defines a line with a certain y-intercept and slope times the explanatory variable. 
IN OTHER WORDS, LINEAR REGRESSION IN ITS MOST BASIC FORM FITS A STRAIGHT LINE TO THE RESPONSE VARIABLE. 

The model is designed to fit a line that minimizes the squared differences (also called errors or residuals.). 
Since linear regression fits data with a line, it is most effective in cases where the response and explanatory variable have a linear relationship.

EXAMPLE : Use linear regression to predict vehicle gas mileage based on vehicle weight. 
import numpy as np
import pandas as pd
from ggplot import mtcars
import matplotlib
import matplotlib.pyplot as plt
import scipy.stats as stats
matplotlib.style.use('ggplot')

mtcars.plot(kind="scatter",
           x="wt",
           y="mpg",
           figsize=(9,9),
           color="black")

from sklearn import linear_model

# Initialize model
regression_model = linear_model.LinearRegression()

# Train the model using the mtcars data
regression_model.fit(X = pd.DataFrame(mtcars["wt"]), 
                     y = mtcars["mpg"])

# Check trained model y-intercept
print(regression_model.intercept_)

# Check trained model coefficients
print(regression_model.coef_)
>>>
37.2851261673
[-5.34447157]

" MPG =  37.2851 - 5.3445 * WT."           


regression_model.score(X = pd.DataFrame(mtcars["wt"]), y = mtcars["mpg"]) #"R-squared" a value that ranges from 0 to 1 which describes the 																				proportion of variance in the response variable that is 																					explained by the model. In this case, car weight explains 																					roughly 75% of the variance in mpg.
>>>0.75283279365826461

The R-squared measure is based on the residuals: differences between what the model predicts for each data point and the actual value of each data point. We can extract the model's residuals by making a prediction with the model on the data and then subtracting the actual value from each prediction:
train_prediction = regression_model.predict(X = pd.DataFrame(mtcars["wt"]))

# Actual - prediction = residuals
residuals = mtcars["mpg"] - train_prediction

residuals.describe()
>>>
count    3.200000e+01
mean    -5.107026e-15
std      2.996352e+00
min     -4.543151e+00
25%     -2.364709e+00
50%     -1.251956e-01
75%      1.409561e+00
max      6.872711e+00
Name: mpg, dtype: float64

R-squared is calculated as 1 - (SSResiduals/SSTotal) were SSResiduals is the sum of the squares of the model residuals and SSTotal is the sum of the squares of the difference between each data point and the mean of the data. We could calculate R-squared by hand like this:
SSResiduals = (residuals**2).sum()
SSTotal = ((mtcars["mpg"] - mtcars["mpg"].mean())**2).sum()

# R-squared
1 - (SSResiduals/SSTotal)
>>>0.75283279365826461


#plot the line it fits on our scatterplot to get a sense of how well it fits the data:
mtcars.plot(kind="scatter",
           x="wt",
           y="mpg",
           figsize=(9,9),
           color="black",
           xlim = (0,7))

# Plot regression line
plt.plot(mtcars["wt"],      # Explanitory variable
         train_prediction,  # Predicted values
         color="blue")

"OUTLIER EFFECT"
mtcars_subset = mtcars[["mpg","wt"]]
super_car = pd.DataFrame({"mpg":50,"wt":10}, index=["super"])
new_cars = mtcars_subset.append(super_car)
# Initialize model
regression_model = linear_model.LinearRegression()
# Train the model using the new_cars data
regression_model.fit(X = pd.DataFrame(new_cars["wt"]), 
                     y = new_cars["mpg"])
train_prediction2 = regression_model.predict(X = pd.DataFrame(new_cars["wt"]))
# Plot the new model
new_cars.plot(kind="scatter",
           x="wt",
           y="mpg",
           figsize=(9,9),
           color="black", xlim=(1,11), ylim=(10,52))
# Plot regression line
plt.plot(new_cars["wt"],     # Explanatory variable
         train_prediction2,  # Predicted values
         color="blue")

"Normality of residuals :  Q-Q (quantile-quantile) plot." : stats.probplot()
plt.figure(figsize=(9,9))

stats.probplot(residuals, dist="norm", plot=plt)

When residuals are normally distributed, they tend to lie along the straight line on the Q-Q plot. In this case residuals appear to follow a slightly non-linear pattern: the residuals are bowed a bit away from the normality line on each end. This is an indication that simple straight line might not be sufficient to fully describe the relationship between weight and mpg.


"Evaluation RMSE"
def rmse(predicted, targets):
    """
    Computes root mean squared error of two numpy ndarrays
    
    Args:
        predicted: an ndarray of predictions
        targets: an ndarray of target values
    
    Returns:
        The root mean squared error as a float
    """
    return (np.sqrt(np.mean((targets-predicted)**2)))

rmse(train_prediction, mtcars["mpg"])
>>>2.9491626859550282
or

from sklearn.metrics import mean_squared_error
RMSE = mean_squared_error(train_prediction, mtcars["mpg"])**0.5

RMSE
>>>2.9491626859550282
------------------------------------------------------
Polynomial Regression
------------------------------------------------------
Variables often exhibit non-linear relationships that can`t be fit well with a straight line. In these cases, we can use linear regression to fit a curved line the data by adding extra higher order terms (squared, cubic, etc.) to the model. A linear regression that involves higher order terms is known as "polynomial regression."

# Initialize model
poly_model = linear_model.LinearRegression()

# Make a DataFrame of predictor variables
predictors = pd.DataFrame([mtcars["wt"],           # Include weight
                           mtcars["wt"]**2]).T     # Include weight squared

# Train the model using the new_cars data
poly_model.fit(X = predictors, 
               y = mtcars["mpg"])

# Check trained model y-intercept
print("Model intercept")
print(poly_model.intercept_)

# Check trained model coefficients (scaling factor given to "wt")
print("Model Coefficients")
print(poly_model.coef_)

# Check R-squared
poly_model.score(X = predictors, 
                 y = mtcars["mpg"])
>>>
Model intercept
49.93081094945181
Model Coefficients
[-13.38033708   1.17108689]
0.8190613581384095 # LR Rsquare >>>0.75283279365826461 Increased so better


# Plot the curve from 1.5 to 5.5
poly_line_range = np.arange(1.5, 5.5, 0.1)

# Get first and second order predictors from range
poly_predictors = pd.DataFrame([poly_line_range,
                               poly_line_range**2]).T

# Get corresponding y values from the model
y_values = poly_model.predict(X = poly_predictors)

mtcars.plot(kind="scatter",
           x="wt",
           y="mpg",
           figsize=(9,9),
           color="black",
           xlim = (0,7))

# Plot curve line
plt.plot(poly_line_range,   # X-axis range
         y_values,          # Predicted values
         color="blue")

preds = poly_model.predict(X=predictors)
rmse(preds , mtcars["mpg"])
>>>2.5233004724610786  # LR RMSE 2.9491626859550282 - Reduced so Better



"OVERFITTING"
# Initialize model
poly_model = linear_model.LinearRegression()

# Make a DataFrame of predictor variables
predictors = pd.DataFrame([mtcars["wt"],           
                           mtcars["wt"]**2,
                           mtcars["wt"]**3,
                           mtcars["wt"]**4,
                           mtcars["wt"]**5,
                           mtcars["wt"]**6,
                           mtcars["wt"]**7,
                           mtcars["wt"]**8,
                           mtcars["wt"]**9,
                           mtcars["wt"]**10]).T     

# Train the model using the new_cars data
poly_model.fit(X = predictors, 
               y = mtcars["mpg"])

# Check trained model y-intercept
print("Model intercept")
print(poly_model.intercept_)

# Check trained model coefficients (scaling factor given to "wt")
print("Model Coefficients")
print(poly_model.coef_)

# Check R-squared
poly_model.score(X = predictors, 
                 y = mtcars["mpg"])

>>>Model intercept
-14921.120647946558
Model Coefficients
[ 6.45813583e+04 -1.20086135e+05  1.26931932e+05 -8.46598480e+04
  3.73155209e+04 -1.10334758e+04  2.16590409e+03 -2.70730550e+02
  1.94974165e+01 -6.15515447e-01]
0.8702106585933677


p_range = np.arange(1.5, 5.45, 0.01)

poly_predictors = pd.DataFrame([p_range, p_range**2, p_range**3,
                              p_range**4, p_range**5, p_range**6, p_range**7, 
                              p_range**8, p_range**9, p_range**10]).T  

# Get corresponding y values from the model
y_values = poly_model.predict(X = poly_predictors)

mtcars.plot(kind="scatter",
           x="wt",
           y="mpg",
           figsize=(9,9),
           color="black",
           xlim = (0,7))

# Plot curve line
plt.plot(p_range,   # X-axis range
         y_values,          # Predicted values
         color="blue")
 Notice how the 10th order polynomial model curves wildly in some places to fit the training data. While this model happens to yield a closer fit to the training data, it will almost certainly fail to generalize well to new data as it leads to absurd predictions such as a car having less than 0 mpg if it weighs 5000lbs.
------------------------------------------------------
Multiple Linear Regression
------------------------------------------------------
When faced with a predictive modeling task, you`ll often have several variables in your data that may help explain variation in the response variable. You can include more explanatory variables in a linear regression model by including more columns in the data frame you pass to the model training function. Let`s make a new model that adds the horsepower variable to our original model:
# Initialize model
multi_reg_model = linear_model.LinearRegression()

# Train the model using the mtcars data
multi_reg_model.fit(X = mtcars.ix[:,["wt","hp"]], 
                     y = mtcars["mpg"])

# Check trained model y-intercept
print(multi_reg_model.intercept_)

# Check trained model coefficients (scaling factor given to "wt")
print(multi_reg_model.coef_)

# Check R-squared
multi_reg_model.score(X = mtcars.ix[:,["wt","hp"]], 
                      y = mtcars["mpg"])
>>>
37.2272701164
[-3.87783074 -0.03177295]

0.8267854518827914

The improved R-squared score suggests horsepower has a linear relationship with mpg. Let`s investigate with a plot:

from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()

data_clean = pd.DataFrame(sc_X.fit_transform(mtcars.iloc[:,1:]), columns = mtcars.iloc[:,1:].columns)
plt.scatter(data_clean.wt,data_clean.mpg,color="black",norm=True)
plt.scatter(data_clean.hp,data_clean.mpg,color="blue",norm=True)  
"""    mtcars.plot(kind="scatter",
           x="hp",
           y="mpg",
           figsize=(9,9),
           color="black")
mtcars.plot(kind="scatter",
           x="hp",
           y="mpg",
           figsize=(9,9),
           color="blue")"""
plt.show()



# Initialize model
multi_reg_model = linear_model.LinearRegression()
# Include squared terms
poly_predictors = pd.DataFrame([mtcars["wt"],
                                mtcars["hp"],
                                mtcars["wt"]**2,
                                mtcars["hp"]**2]).T
# Train the model using the mtcars data
multi_reg_model.fit(X = poly_predictors, 
                    y = mtcars["mpg"])
# Check R-squared
print("R-Squared")
print( multi_reg_model.score(X = poly_predictors , 
                      y = mtcars["mpg"]) )
# Check RMSE
print("RMSE")
print(rmse(multi_reg_model.predict(poly_predictors),mtcars["mpg"]))


The new R-squared and lower RMSE suggest this is a better model than any we made previously and we wouldn`t be too concerned about overfitting since it only includes 2 variables and 2 squared terms. Note that when working with multidimensional models, it becomes difficult to visualize results, so you rely heavily on numeric output.
We could continue adding more explanatory variables in an attempt to improve the model. Adding variables that have little relationship with the response or including variables that are too closely related to one another can hurt your results when using linear regression. You should also be wary of numeric variables that take on few unique values since they often act more like categorical variables than numeric ones.

********************************************************************************************************************************************
******************************************************************************************************************************************************

LinearRegressionDiagnosticPlots.odt
Diagnostic Plots
	-residplot
	-probplot

 1. "https://zhiyzuo.github.io/Linear-Regression-Diagnostic-in-Python/ "
 2. "https://medium.com/@neuralnets/statistical-data-visualization-series-with-python-and-seaborn-for-data-science-5a73b128851d "
 3. "https://medium.com/@emredjan/emulating-r-regression-plots-in-python-43741952c034 "
 4. "https://robert-alvarez.github.io/2018-06-04-diagnostic_plots/ "

----------------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------------
"https://www.youtube.com/user/jbstatistics/videos"
Outliers, Leverage, Influence  -  https://www.youtube.com/watch?v=xc_X9GFVuVU
Leverage = 1/n + (xᵢ - x̄ )/(TSS)
Influence = \Cook`s distance

from statsmodels.stats.outliers_influence import summary_table

diabetes = datasets.load_diabetes()
X = diabetes.data
y = diabetes.target
X2 = sm.add_constant(X)
est = sm.OLS(y, X2)
est2 = est.fit()
st, data, ss2 = summary_table(est2, alpha=0.05)

******************************************************************************************************************************************************
----------------------------------------------------------------------------------------------------------------------------------------
Regression Overview with \"ASSUMPTIONS" Explained
"http://www.zstatistics.com/regression"
"http://www.zstatistics.com/videos"
1.	The true relationship between dependent y and predictor x is linear - Linearity
2.	The model errors are statistically independent
3.	The errors are normally distributed with a 0 mean and constant standard deviation - Normality of errors
4.	The predictor x is non-stochastic and is measured error-free
When deriving regression parameters, we make all the four assumptions mentioned above. If any of the assumptions is violated, the model would be misleading.
Interpreting our linear regression model includes following parameters/outputs/analysis.

			Coeff 	Std Error 	t-stat 	P-value				
Intercept   3.0605	1.4024		2.1823	0.02909
X 			0.7393	0.2308 		3.20339 0.00136


"If the ASSUMPTIONS are not met then these above parameters become Un-RELIABLE"

"http://r-statistics.co/Assumptions-of-Linear-Regression.html"
"https://www.statisticssolutions.com/assumptions-of-linear-regression/"
@L M E A N H 

1. LINEARITY  
	Correct functional form
	Y = B₀ + B₁ X₁ + ε
	Can be improved by adding polynomial 
	Y = B₀ + B₁ X₁ + B₂ X₁² + 	ε
	
	- Coeff and Std Error becomes unreliable and thus t-stat and P-value become unreliable.

	- Detection
		1. residplot()
			Should have an even spread across X
		2. Likelihood Ratio-(LR) test
	
    - Remedies
    	1. Get the specifications correct using Trial and Error 

2. HOMOSKEDASTICITY : CONSTANT ERROR VARIANCE i.e NO HETEROSKEDASTICITY 
			Y 	 = B₀ + B₁ 	X₁ 		+ ε
	Expenditure  = B₀ + B₁ (Income) + ε

	regplot() or residplot()
		In both the observation points aroud the line should have a constant variance throughout, as in case of income and Expenditure it isn`t a case since for low income group the variance is not much since they don't have a choice of spending very high or very low- (So less variance aroud y axis) But for higher income group they can either spend alot or some can even spend ver less. Range of expenditure increases. this problem means haveing HETEROSKEDASTICITY - (Hetero = Numerous, Skedasticity = Variance) i.e NON CONSTANT thus NO HOMOSKEDASTICITY, thus Issue.
	
	- The Standard Error is unreliable although Coeff can be relied upon since they still are the best fit for the data.

	- Detection using
		1. GOldfeldt-Quant test
		2. Breusch-Pagan test 
			(Both these tests give a p-vale that can be use to detect if HETEROSKEDASTICITY exists or not.)

	 - Remedies
    	1. White`s Standard errors
    	2. Weighted least squares
    	3. Log both variables - (apply transformation)


3. INDEPENDENCE OF ERRORS : NO AUTOCORRELATION
			Y 	 = B₀ + B₁ 	X₁ 		+ ε
	Stock Index  = B₀ + B₁ (Time) 	+ ε

	Can occur in data where there is some natural order in your data variables - ("Time series data")

	- The Standard Error is unreliable although Coeff can be relied upon since they still are the best fit for the data.
	- Detection using
		1. Durbin-Watson test - statsmodels.stats.stattools.durbin_watson
		2. Breusch-Godfrey test - statsmodels.stats.diagnostic.acorr_breusch_godfrey
			(Both these tests give a p-vale that can be use to detect if AUTOCORRELATION exists or not.)
		3. Wald–Wolfowitz runs test - statsmodels.sandbox.stats.runs


	- Remedies
    	1. Investigate Ommited variables
    	2. Generalised difference Equation
    		Cochrane-Orchutt or AR(1) methods



4. NORMALITY OF ERRORS : NORMALLY DISTRIBUTED ERRORS
						Y 	  = B₀ + B₁ 	X₁ 			+ ε
	Medical Insurance Payout  = B₀ + B₁ (Customer Age) 	+ ε

	Can occur when a majority of dependent variable lies of y = 0 , like in our case Insurance payout rarely happens and most people, irrespective of their Age-(X-axis) don`t need the payout - (No health issue ever). in that case the Residplot does not have a" BELL shaped distribution" around the axis(most amount of data should lie around the line) a Right Skewed distribution exists.
	regplot() or residplot() 
	- If normality is violated and n is small, the Std Errors are affected.

	- Detection using
		1. HistoGram or QQplot stats.probplot() or a normal distribution plot to see if bell shap exists
		2. Shapro-Wilk test 
		3. Komolforov-Smirnov test
		4. Anderson-Darling test
			(Both these tests give a p-vale that can be use to detect if AUTOCORRELATION exists or not.)

	- Remedies
    	1. Log both variables - (apply transformation)
    	2. Get more observations

5. MULTICOLLINEARITY : TRUELY INDEPENDENT X TERMS (INDEPENDENT VARIABLES ARE TRUELY INDEPENDENT)
				Y 	  = B₀ + B₁ 	X₁ 				+ B₂ X₂					+ ε
	Motor Accidents   = B₀ + B₁ (Number of Cars) 	+ B₂ (Num of Residents)	+ ε

	Can occur when X variables are themselves Co-related, as  Num of Residents increase Number of Cars
	
	- Coeff and Std Error of affected variables becomes unreliable and thus t-stat and P-value become unreliable.

	- Detection using
		1. Look at correlation-(p) between X₁ and  X₂
		2. Look at Variance Inflation Factor

	- Remedies
    	1. "Remove one of variable"



6. EXOGENEITY : OMITTED VARIABLE BIAS
			Y 	  	  = B₀ + B₁ 	X₁ 				+ ε
			Salary    = B₀ + B₁ (Years of Education)+ ε

	"Remove one of variable" can cause in EXOGENEITY 
	Socio-economic status -(Richness) affects both X and Y variables, thus could cause ommited variable bias.
	Socio-economic status would affect ε in the model, thus, Education is no longer wholly exogenous as it can be explained in part by the error term.
	
	- Issue is that the model can only be used For prediction and not infer causation.

	- Detection using
		1. Own intution
		2. Checking correlation 

	- Remedies
    	1. Remove one of variable	
7. MULTIVARIATE NORMALITY
******************************************************************************************************************************************************
----------------------------------------------------------------------------------------------------------------------------------------
"https://www.analyticsvidhya.com/blog/2016/12/45-questions-to-test-a-data-scientist-on-regression-skill-test-regression-solution/"
@ R-Squared vs Adjusted R-Squared
“R squared” individually can’t tell whether a variable is significant or not because each time when we add a feature, “R squared” can either increase or stay constant. But, it is not true in case of “Adjusted R squared” (increases when features found to be significant).
Each time when you add a feature, R squared always either increase or stays constant, but it is not true in case of Adjusted R squared. If it increases, the feature would be significant.

@ We always consider residual as vertical offsets. Perpendicular offset are useful in case of PCA.

@ Lasso Regression
	Q16. Suppose we fit “Lasso Regression” to a data set, which has 100 features (X1,X2…X100).  Now, we rescale one of these feature by multiplying with 10 (say that feature is X1),  and then refit Lasso regression with the same regularization parameter.
	Big feature values ⇒ smaller coefficients ⇒ less lasso penalty =⇒ more likely to be kept

@ Ridge regression will use all predictors in final model whereas Lasso regression can be used for feature selection because coefficient values can be zero. For more detail 
	'https://discuss.analyticsvidhya.com/t/difference-between-ridge-regression-and-lasso-and-its-effect/3000'

@ R Squared, Adjusted R Squared, RMSE / MSE / MAE, \f statistics for regression

@ We can also compute the coefficient of linear regression with the help of an analytical method called NORMAL EQUATION. In it :
	-	We don’t have to choose the learning rate
	-	It becomes slow when number of features is very large
	-	No need to iterate
	"NORMAL EQUATION VS GRADIENT DESCENT"
		Gradient Descent:
	-	need to choose learning rate α
	-	need to do many iterations
	-	works well with large n
		
		Normal Equation:
	-	don`t need to choose α
	-	don`t need to iterate - computed in one step
	-	slow if n is large (n⩾104)
	-	need to compute (XTX)−1 - very slow
		if (XTX) is not-invertible - we have problems

"https://stattrek.com/multiple-regression/regression-coefficients.aspx"
@ Simple Linear Regression
	ŷ = b0 + b1x
	b1 = Σ [ (xi - x)(yi - y) ] / Σ [ (xi - x)2]
	b0 = y - b1 * x


@ Multiple Linear Regression
	ŷ = b0 + b1x1 + b2x2 + … + bk-1xk-1 + bkxk
	Y = Xb
	X`Y = X`Xb 
		Here, matrix X` is the transpose of matrix X. To solve for regression coefficients, simply pre-multiply by the inverse of X`X:
	(X`X)-1X`Xb = (X`X)-1X`Y
	b 			= (X`X)-1X`Y
"https://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression/"


@ The expected value of Y is a linear function of the X(X1,X2….Xn) variables and regression line is defined as:
	Y = β0 +  β1 X1 + β2 X2……+ βn Xn

	1.The expected value of Y is a linear function of the X variables. This means:
        1. If X i changes by an amount ∆X i , holding other variables fixed, then the expected value of Y changes by a proportional amount β i ∆X i , for some constant β i (which in general could be a positive or negative number).
        2. The value of β i is always the same, regardless of values of the other X’s.
        3. The total effect of the X’s on the expected value of Y is the sum of their separate effects.
    2. The unexplained variations of Y are independent random variables (in particular, not “auto correlated” if the variables are time series)
    3. They all have the same variance (“homoscedasticity”).
    4. They are normally distributed.

@ In simple linear regression there is one independent variable so 2 coefficients (Y=a+bx).

@ SUM OF RESIDUALS ALWAYS ZERO.

@ If two variables are correlated is it necessary that they have a linear relationship? NO 

@ Correlated variables can have zero correlation coeffficient
"https://stats.stackexchange.com/questions/179511/why-zero-correlation-does-not-necessarily-imply-independence"

@ Adding more FEATURES to model will always increase the training accuracy i.e. low bias. But testing accuracy increases if feature is found to be significant.

@ Symmetric relationship : CORRELATION vs REGRESSION
 ✔   1. Correlation is a statistic metric that measures the linear association between two variables. It treats y and x symmetrically.
 ✖   2. Regression is setup to predict y from x. The relationship is not symmetric.

@ Skewness
  The skewness is not directly related to the relationship between the mean and median.
  Skewness measures the skew or asymmetry of a distribution while kurtosis measures the "peakedness" of a distribution. 

@ Anscombes quartet
  "https://en.wikipedia.org/wiki/Anscombe's_quartet"
  Francis John Frank Anscombe

@ RIDGE REGRESSION
	Suppose you have fitted a complex regression model on a dataset. Now you are using RIDGE REGRESSION with tuning parameter lambda to reduce its complexity. Choose the option(s) below which describes relationship of bias and variance with lambda.
			If lambda is very large it means model is less complex. So in this case bias is high and variance in low.
			If lambda is very small it means model is complex.So in this case bias is low and variance is high because model will overfit the data.
	Specifically, we can see that when lambda is 0, we get our least square solution. When lambda goes to infinity, we get very, very small coefficients approaching 0.

@ Residual plots
	Q38
	There should not be any relationship between predicted values and residuals. If there exist any relationship between them means model has not perfectly capture the information in data.
	LinearRegressionDiagnosticPlots.odt

@ Closed form solutions
  The Lasso does not admit a closed-form solution. The L1-penalty makes the solution non-linear. So we need to approximate the solution.
  "http://statweb.stanford.edu/~tibs/sta305files/Rudyregularization.pdf"

@ If we change the input variable by one unit. How much output variable will change?
  Equation for simple linear regression: Y=a+bx. Now if we increase the value of x by 1 then the value of y would be a+b(x+1) i.e. value of y will get incremented by b.

@ Sigmoid function is used to convert output probability between [0,1] in logistic regression.

@ Partial derivative of the cost functions w.r.t weights(coefficients) in linear-regression and logistic-regression is "SAME"

@ One_vs_rest method One-vs-rest method
  If there are n classes, then n separate logistic regression has to fit, where the probability of each category is predicted over the rest of the categories combined.
  Take a example of 3-class(-1,0,1) classification. Then need to train 3 logistic regression classifiers.
   	-1 vs  0 and 1
   	 0 vs -1 and 1
   	 1 vs  0 and -1



***************************************************************ANDREW NG***********************************************************************************
"https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1/"
-----------------------------------------------------------------------------------
ANDREWG COURSERA - LINEAR REGRESSION
https://www.coursera.org/learn/machine-learning/home/week/3

HYPOTHESIS
COST FUNCTION
GRADIENT DESCENT
	GRADIENT DESCENT INTUITION
	GRADIENT DESCENT FOR LINEAR REGRESSION
REGULARIZATION
	REGULARIZATION NORMAL EQUATION

-----------------------------------------------------------------------------------
HYPOTHESIS
					hᶱ(x) = θ₀ + θ₁x

-----------------------------------------------------------------------------------
COST FUNCTION
						1    m               	  1    m
	J(θ₀,θ₁)     =	   ----  ∑	(ŷᵢ - yᵢ)²	=	 ----  ∑	( hᶱ(xᵢ) - yi )²
			  		    2m  ᶦ⁼¹              	  2m  ᶦ⁼¹  


-----------------------------------------------------------------------------------
GRADIENT DESCENT 

The gradient descent algorithm is:

repeat until convergence:
					   ∂
		θⱼ:=  θⱼ − α ----- J(θ₀,θ₁)
					  ∂θⱼ
			where,
					ⱼ =₀,₁ represents the feature index number.

At each iteration j, one should simultaneously update the parameters θ₁,θ₂,θ₃.... θₙ
Updating a specific parameter prior to calculating another one on the jᵗʰ iteration would yield to a wrong implementation.

-----------------------------------------------------------------------------------
GRADIENT DESCENT INTUITION
1.
Regardless of the slope`s sign for       d            
	  									---  J(θ₁)
	   									dθ₁		     , θ₁ eventually converges to its minimum value. 


The following graph shows that when the slope is negative, the value of θ₁ increases and when it is positive, the value of θ₁decreases.

---------------------------------
2.
How does gradient descent converge with a fixed step size "α" ?
	The intuition behind the convergence is that 
													d
	  											   ---  J(θ₁)
	   											   dθ₁
​	approaches 0 as we approach the bottom of our convex function. At the minimum, the derivative will always be 0 and thus we get:

	θ₁:= θ₁ − α ∗ 0


-----------------------------------------------------------------------------------
GRADIENT DESCENT FOR LINEAR REGRESSION

We can substitute our actual cost function and our actual hypothesis function and modify the equation to :
Repeat until convergence: 
	
	{					1   m
		θ₀:=  θ₀  -  α ---  ∑	( hᶱ(xᵢ) - yi )
						m  ᶦ⁼¹ 
		
						1   m
		θ₁:=  θ₁  -  α ---  ∑	( hᶱ(xᵢ) - yi ) (xᵢ)
						m  ᶦ⁼¹ 
	}
The point of all this is that if we start with a guess for our hypothesis and then repeatedly apply these gradient descent equations, our hypothesis will become more and more accurate.
	

-----------------------------------------------------------------------------------
REGULARIZATION

We will modify our gradient descent function to separate out θ₀ from the rest of the parameters bcz we do not want to penalize θ₀
​	 .
Repeat until convergence: 
	
	{					1   m
		θ₀:=  θ₀  -  α ---  ∑	( hᶱ(xᶦ) - yᶦ )
						m  ᶦ⁼¹ 
					   _                                               _
					  |	  	 1   m 								λ       |
		θⱼ:=  θⱼ  -  α|	 (	---  ∑	( hᶱ(xᶦ) - yᶦ ) (xⱼ) )  +  --- θⱼ   |
					  |_  	 m  ᶦ⁼¹ 							m      _|
	}


		Can also be written as :

					 λ		    1    m 							
		θⱼ:=  θⱼ - α--- θⱼ - α  ---  ∑	( hᶱ(xᶦ) - yᶦ ) (xⱼᶦ) 
					 m		 	 m  ᶦ⁼¹ 						

					  λ		    1   m 							
		θⱼ:=  θⱼ(1- α---) -  α ---  ∑	( hᶱ(xᶦ) - yᶦ ) (xⱼᶦ) 
					  m		 	m  ᶦ⁼¹ 						

-----------------------------------------------------------------------------------
REGULARIZATION NORMAL EQUATION
Now let`s approach regularization using the alternate method of the non-iterative normal equation.

To add in regularization, the equation is the same as our original, except that we add another term inside the parentheses:

		θ = (Xᵀ X + λ⋅L)−¹ Xᵀy
		where  L=⎡0						  ⎤
				 ⎢	1					  ⎢
				 ⎢		1 				  ⎢
				 ⎢			.  			  ⎢
				 ⎢				.    	  ⎢
				 ⎢					.     ⎢
				 ⎣						1 ⎦
L is a matrix with 0 at the top left and 1's down the diagonal, with 0's everywhere else. It should have dimension (n+1)×(n+1). Intuitively, this is the identity matrix (though we are not including x₀), multiplied with a single real number λ.

Recall that if m < n, then Xᵀ X is non-invertible. However, when we add the term λ⋅L, then Xᵀ X + λ⋅L becomes invertible.


****************************************************CODE****************************************************************************
