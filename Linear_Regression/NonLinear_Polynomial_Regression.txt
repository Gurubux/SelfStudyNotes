------------------------------------------------------
POLYNOMIAL REGRESSION
------------------------------------------------------
Variables often exhibit non-linear relationships that can`t be fit well with a straight line. In these cases, we can use linear regression to fit a curved line the data by adding extra higher order terms (squared, cubic, etc.) to the model. A linear regression that involves higher order terms is known as "polynomial regression."

# Initialize model
poly_model = linear_model.LinearRegression()

# Make a DataFrame of predictor variables
predictors = pd.DataFrame([mtcars["wt"],           # Include weight
                           mtcars["wt"]**2]).T     # Include weight squared

# Train the model using the new_cars data
poly_model.fit(X = predictors, 
               y = mtcars["mpg"])

# Check trained model y-intercept
print("Model intercept")
print(poly_model.intercept_)

# Check trained model coefficients (scaling factor given to "wt")
print("Model Coefficients")
print(poly_model.coef_)

# Check R-squared
poly_model.score(X = predictors, 
                 y = mtcars["mpg"])
>>>
Model intercept
49.93081094945181
Model Coefficients
[-13.38033708   1.17108689]
0.8190613581384095 # LR Rsquare >>>0.75283279365826461 Increased so better


# Plot the curve from 1.5 to 5.5
poly_line_range = np.arange(1.5, 5.5, 0.1)

# Get first and second order predictors from range
poly_predictors = pd.DataFrame([poly_line_range,
                               poly_line_range**2]).T

# Get corresponding y values from the model
y_values = poly_model.predict(X = poly_predictors)

mtcars.plot(kind="scatter",
           x="wt",
           y="mpg",
           figsize=(9,9),
           color="black",
           xlim = (0,7))

# Plot curve line
plt.plot(poly_line_range,   # X-axis range
         y_values,          # Predicted values
         color="blue")

preds = poly_model.predict(X=predictors)
rmse(preds , mtcars["mpg"])
>>>2.5233004724610786  # LR RMSE 2.9491626859550282 - Reduced so Better



"OVERFITTING"
# Initialize model
poly_model = linear_model.LinearRegression()

# Make a DataFrame of predictor variables
predictors = pd.DataFrame([mtcars["wt"],           
                           mtcars["wt"]**2,
                           mtcars["wt"]**3,
                           mtcars["wt"]**4,
                           mtcars["wt"]**5,
                           mtcars["wt"]**6,
                           mtcars["wt"]**7,
                           mtcars["wt"]**8,
                           mtcars["wt"]**9,
                           mtcars["wt"]**10]).T     

# Train the model using the new_cars data
poly_model.fit(X = predictors, 
               y = mtcars["mpg"])

# Check trained model y-intercept
print("Model intercept")
print(poly_model.intercept_)

# Check trained model coefficients (scaling factor given to "wt")
print("Model Coefficients")
print(poly_model.coef_)

# Check R-squared
poly_model.score(X = predictors, 
                 y = mtcars["mpg"])

>>>Model intercept
-14921.120647946558
Model Coefficients
[ 6.45813583e+04 -1.20086135e+05  1.26931932e+05 -8.46598480e+04
  3.73155209e+04 -1.10334758e+04  2.16590409e+03 -2.70730550e+02
  1.94974165e+01 -6.15515447e-01]
0.8702106585933677


p_range = np.arange(1.5, 5.45, 0.01)

poly_predictors = pd.DataFrame([p_range, p_range**2, p_range**3,
                              p_range**4, p_range**5, p_range**6, p_range**7, 
                              p_range**8, p_range**9, p_range**10]).T  

# Get corresponding y values from the model
y_values = poly_model.predict(X = poly_predictors)

mtcars.plot(kind="scatter",
           x="wt",
           y="mpg",
           figsize=(9,9),
           color="black",
           xlim = (0,7))

# Plot curve line
plt.plot(p_range,   # X-axis range
         y_values,          # Predicted values
         color="blue")
 Notice how the 10th order polynomial model curves wildly in some places to fit the training data. While this model happens to yield a closer fit to the training data, it will almost certainly fail to generalize well to new data as it leads to absurd predictions such as a car having less than 0 mpg if it weighs 5000lbs.


 Let me answer two important questions here: 

First, “How can I know if a problem is linear or non-linear in an easy way?”
To answer this question, we have to do two things:
  The first is to visually figure out if the relation is linear or non-linear. It’s best to plot bivariate plots of output variables with each input variable.
  Also, you can calculate the correlation coefficient between independent and dependent variables, and if for all variables it is 0.7 or higher there is a linear tendency, and, thus, it’s not appropriate to fit a non-linear regression.

  The second thing we have to do is to use non-linear regression instead of linear regression when we cannot accurately model the relationship with linear parameters.

The second important questions is, “How should I model my data, if it displays non-linear on a scatter plot?”
  Well, to address this, you have to use either a polynomial regression, use a non-linear regression model, or "transform" your data, which is not in scope for this course.
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
NON-LINEAR REGRESSION - "https://github.com/Gurubux/CognitiveClass-ML/blob/master/Course_MachineLearningWithPython/3-Regression/3.6Non-LinearRegression/CC_3_6_NonLinearRegression.ipynb"
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
1. Non-linear functions can have elements like exponentials, logarithms, fractions, and others. For example: 
    y = log(x)
  Please consider that instead of x, we can use X, which can be polynomial representation of the x`s. In general form it would be written as
    y = log(X)
  X = np.arange(-5.0, 5.0, 0.1)

  Y = np.log(X)
  
  plt.plot(X,Y) 
  plt.ylabel('Dependent Variable')
  plt.xlabel('Indepdendent Variable')
  plt.show()


2. Or even, more complicated such as : 
    y = log(a x^3 + b x^2 + c x + d)

3. Cubic Function
    y = a x^3 + b x^2 + c x + d 
  x = np.arange(-5.0, 5.0, 0.1)

  ##You can adjust the slope and intercept to verify the changes in the graph
  y = 1*(x**3) + 1*(x**2) + 1*x + 3
  y_noise = 20 * np.random.normal(size=x.size)
  ydata = y + y_noise
  plt.plot(x, ydata,  'bo')
  plt.plot(x,y, 'r') 
  plt.ylabel('Dependent Variable')
  plt.xlabel('Indepdendent Variable')
  plt.show()



4. Quadratic
    Y = X^2

  x = np.arange(-5.0, 5.0, 0.1)

  ##You can adjust the slope and intercept to verify the changes in the graph

  y = np.power(x,2)
  y_noise = 2 * np.random.normal(size=x.size)
  ydata = y + y_noise
  plt.plot(x, ydata,  'bo')
  plt.plot(x,y, 'r') 
  plt.ylabel('Dependent Variable')
  plt.xlabel('Indepdendent Variable')
  plt.show()

5. Exponential
  An exponential function with base c is defined by 
    Y = a + b c^X
      where b ≠0, c > 0, c ≠1, and x is any real number. The base, c, is constant and the exponent, x, is a variable.
  X = np.arange(-5.0, 5.0, 0.1)
  
  ##You can adjust the slope and intercept to verify the changes in the graph
  
  Y= np.exp(X)
  
  plt.plot(X,Y) 
  plt.ylabel('Dependent Variable')
  plt.xlabel('Indepdendent Variable')
  plt.show()


6. Sigmoidal/Logistic
              b
  Y =   a + -----------
        1 + c⁽ˣ⁻ᵈ⁾

7. The formula for the logistic function is the following:
              1
  Y =     -----------
              β₁(X⁻β₂)
        1 + e
  β₁ : Controls the curve`s steepness.
  β₂ : Slides the curve on the x-axis.
  
  def sigmoid(x, Beta_1, Beta_2):
    y = 1 / (1 + np.exp(-Beta_1*(x-Beta_2)))
    return y

  #logistic function
  Y_pred = sigmoid(x_data, beta_1 , beta_2)
  
  #plot initial prediction against datapoints
  plt.plot(x_data, Y_pred*15000000000000.)
  plt.plot(x_data, y_data, 'ro')


----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
https://github.com/Gurubux/CognitiveClass-ML/blob/master/Course_MachineLearningWithPython/3-Regression/3.6Non-LinearRegression/CC_3_6_NonLinearRegression.ipynb

How we find the best parameters for our fit line?
we can use curve_fit which uses non-linear least squares to fit our sigmoid function, to data. Optimal values for the parameters so that the sum of the squared residuals of sigmoid(xdata, *popt) - ydata is minimized.

"popt" is our optimized parameter/s.


  from scipy.optimize import curve_fit
  popt, pcov = curve_fit(sigmoid, xdata, ydata)
  #print the final parameters
  print(" beta_1 = %f, beta_2 = %f" % (popt[0], popt[1]))
>>>beta_1 = 690.453017, beta_2 = 0.997207

#Now we plot our resulting regresssion model.
  x = np.linspace(1960, 2015, 55)
  x = x/max(x)
  plt.figure(figsize=(8,5))
  y = sigmoid(x, *popt)
  plt.plot(xdata, ydata, 'ro', label='data')
  plt.plot(x,y, linewidth=3.0, label='fit')
  plt.legend(loc='best')
  plt.ylabel('GDP')
  plt.xlabel('Year')
  plt.show()



---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
*************************************************************************************************************************************
  POLYNOMIAL REGRESSION AND PIPELINES - "https://github.com/Gurubux/CognitiveClass-ML/tree/master/2-AppliedDataScienceWithPython/2-2-DataAnalysisWithPython"
*************************************************************************************************************************************
Polynomial regression is a particular case of the general linear regression model or multiple linear regression models.
We get non-linear relationships by squaring or setting higher-order terms of the predictor variables.
*NOTE* "Although the predictor variables of Polynomial linear regression are not linear the relationship between the parameters or coefficients is linear. "
There are different orders of polynomial regression:
₁₂₃
        Quadratic - 2nd order
      𝑌ℎ𝑎𝑡=𝑎+𝑏₁𝑋²+𝑏₂𝑋²
       
        Cubic - 3rd order
      𝑌ℎ𝑎𝑡=𝑎+𝑏₁𝑋2+𝑏₂𝑋²+𝑏₃𝑋³

        Higher order:
      𝑌=𝑎+𝑏₁𝑋+𝑏₂𝑋²+𝑏₃𝑋³....

x = df['highway-mpg']
y = df['price']

# Here we use a polynomial of the 3rd order (cubic) 
f = np.polyfit(x, y, 3)
p = np.poly1d(f)
print(p)
>>>              
  -1.557 𝑋³ + 204.8 𝑋² - 8965 x + 1.379e+05

PlotPolly(p, x, y, 'highway-mpg')

def PlotPolly(model, independent_variable, dependent_variabble, Name):
    x_new = np.linspace(15, 55, 100)
    y_new = model(x_new)

    plt.plot(independent_variable, dependent_variabble, '.', x_new, y_new, '-')
    plt.title('Polynomial Fit with Matplotlib for Price ~ Length')
    ax = plt.gca()
    ax.set_facecolor((0.898, 0.898, 0.898))
    fig = plt.gcf()
    plt.xlabel(Name)
    plt.ylabel('Price of Cars')

    plt.show()
    plt.close()

"We can already see from plotting that this polynomial model performs better than the linear model. This is because the generated polynomial function "hits" more of the data points."


The analytical expression for Multivariate Polynomial function gets complicated. For example, the expression for a second-order (degree=2)polynomial with two variables is given by:

    𝑌ℎ𝑎𝑡=𝑎+𝑏1𝑋1+𝑏2𝑋2+𝑏3𝑋1𝑋2+𝑏4𝑋21+𝑏5𝑋22
Create 11 order polynomial model with the variables x and y from above
# Write your code below and press Shift+Enter to execute 
f_1 = np.polyfit(x, y, 11)
p_1 = np.poly1d(f_1)
print(p_1)
PlotPolly(p, x, y, 'highway-mpg')
------------------------------------------------------------
We can perform a polynomial transform on multiple features. First, we import the module:
------------------------------------------------------------
from sklearn.preprocessing import PolynomialFeatures

pr=PolynomialFeatures(degree=2) #We create a PolynomialFeatures object of degree 2:
>>>PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)
Z_pr=pr.fit_transform(Z)

Z.shape
>>>(201, 4)
Z_pr.shape
>>>(201, 15)
------------------------------------------------------------
PIPELINES
Data Pipelines simplify the steps of processing the data. We use the module Pipeline to create a pipeline. We also use StandardScaler as a step in our pipeline.
------------------------------------------------------------
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
Input=[('scale',StandardScaler()), ('polynomial', PolynomialFeatures(include_bias=False)), ('model',LinearRegression())]
pipe=Pipeline(Input)
>>>
  Pipeline(memory=None,
         steps =[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)),
               ('polynomial', PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)), 
               ('model', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False))
              ])

pipe.fit(Z,y)
>>> 
  Pipeline(memory=None,
         steps =[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), 
                ('polynomial', PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)), 
                ('model', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False))])

ypipe=pipe.predict(Z)
ypipe[0:4]
>>> array([13102.74784201, 13102.74784201, 18225.54572197, 10390.29636555])

distplot()
plt.figure(figsize=(width, height))


ax1 = sns.distplot(df['price'], hist=False, color="r", label="Actual Value")
sns.distplot(ypipe, hist=False, color="b", label="Fitted Values" , ax=ax1)


plt.title('Actual vs Fitted Values for Price')
plt.xlabel('Price (in dollars)')
plt.ylabel('Proportion of Cars')

plt.show()
plt.close()

------------------------------------------------------------
# Write your code below and press Shift+Enter to execute 
pipe_2 = Pipeline([('scale',StandardScaler()), ('model',LinearRegression())])
pipe_2.fit(Z,y)
ypipe_2=pipe_2.predict(Z)