Supervised Machine Learning: Classification
https://towardsdatascience.com/supervised-machine-learning-classification-5e685fe18a6d

INTRO TO LOGISTIC REGRESSION
1. What is  logistic regression?
2. What kind of problems solved?
3. Which situations we should use Logistic regression?

1. What is  logistic regression?
	Logistic regression is a statistical and machine learning technique for classifying records of a dataset, based on the values of the input fields.
	Let’s say we have a telecommunication dataset that we’d would like to analyze, in order	 to understand which customers might leave us next month.	 This is historical customer data where each row represents one customer.
	You’ll use the dataset to build a model based on historical records and use it to predict the future churn within the customer group.
	INDEPENDENT VARIABLE : 
		In logistic regression, independent variables should be continuous; if categorical, they should be dummy or indicator-coded. This means we have to transform them to some continuous value.
	DEPENDENT VARIABLE : 
		logistic regression can be used for both binary classification and multiclass classification

2. What kind of problems solved?
	a. To predict the "probability" of a person having a heart attack within a specified time period, based on our knowledge of the person's age, sex, and body mass index.
	b. To predict the chance of mortality in an injured patient, or to "predict" whether a patient has a given disease, such as diabetes, based on observed characteristics of that patient, such as weight, height, blood pressure, and results of various blood tests, and so on.
	c. In a marketing context, we can use it to predict the "likelihood" of a customer purchasing a product or halting a subscription, as we’ve done in our churn example.
	d. To predict the "probability" of failure of a given process, system, or product.
	e. To predict the "likelihood" of a homeowner defaulting on a mortgage.

	\Notice that in all of these examples, not only do we predict the class of each case, we also measure the probability of a case belonging to a specific class.

3. Which situations we should use Logistic regression?
	Four situations in which Logistic regression is a good candidate:
		a. When the "target field in your data is categorical", or specifically, is binary, such as 0/1, yes/no, churn or no churn, positive/negative, and so on.
		
		b. When you "need the probability" of your prediction, for example, if you want to know what the probability is, of a customer buying a product. Logistic regression returns a probability score between 0 and 1 for a given sample of data. In fact, logistic regressing predicts the probability of that sample, and we map the cases to a discrete class based on that probability.
		
		c. If your data is "linearly separable".
				The decision boundary of logistic regression is a line or a plane or a hyper-plane. A classifier will classify all the points on one side of the decision boundary as belonging to one class and all those on the other side as belonging to the other class. For example, if we have just two features (and are not applying any polynomial processing), we can obtain an inequality like θ_0+ θ_1 x_1+ θ_2 x_2 > 0, which is a half-plane, easily plottable.
				Please note that in using logistic regression, we can also achieve a complex decision boundary using polynomial processing as well, which is out of scope here.

		d. When you need to "understand the IMPACT" of a feature.
				You can select the best features based on the statistical significance of the logistic regression model coefficients or parameters.
				That is, after finding the optimum parameters, a feature x with the weight θ_1 close to 0, has a smaller effect on the prediction, than features with large absolute values of θ_1. Indeed, it allows us to understand the impact an independent variable has on the dependent variable while controlling other independent variables.

					ŷ    = 		P(y = 1|X)
				P(y=0|X) = 1 -  P(y = 1|X)



LOGISTIC VS LINEAR REGRESSION
Steps:
"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/4-Classification/LogR_steps.PNG"

Choosing θ :
	Gradient Descent

Stopping Iteration :
	Satisfactory Accuracy Score.


LAB: LOGISTIC REGRESSION
While Linear Regression is suited for estimating continuous values (e.g. estimating house price), it is not the best tool for predicting the class of an observed data point. In order to estimate the class of a data point, we need some sort of guidance on what would be the most probable class for that data point. For this, we use Logistic Regression.

The objective of Logistic Regression algorithm, is to find the best parameters θ, for ℎ_θ(𝑥) = 𝜎({θ^TX}), in such a way that the model best predicts the class of each case.



# write your code here
parameters = {'C':[0.01,0.1,1,10],'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}
from sklearn.model_selection import GridSearchCV
#LR_1 = LogisticRegression(C=0.1, solver='liblinear').fit(X_train,y_train)
LR_1 = LogisticRegression()
cv = GridSearchCV(LR_1,parameters, cv = 5)
cv.fit(X_train,y_train)
print_results(cv)

print(log_loss(y_test, cv.best_estimator_.predict_proba(X_test)))
print (classification_report(y_test, cv.best_estimator_.predict(X_test)))
>>>

BEST PARAMS: {'C': 0.1, 'solver': 'liblinear'}

LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',
          tol=0.0001, verbose=0, warm_start=False)
                                params  mean_test_score
7    {'C': 0.1, 'solver': 'liblinear'}          0.76875
2   {'C': 0.01, 'solver': 'liblinear'}          0.76250
10     {'C': 1, 'solver': 'newton-cg'}          0.75000
14          {'C': 1, 'solver': 'saga'}          0.75000
5    {'C': 0.1, 'solver': 'newton-cg'}          0.75000


0.731 (+/-0.018) for {'C': 0.01, 'solver': 'newton-cg'}
0.731 (+/-0.018) for {'C': 0.01, 'solver': 'lbfgs'}
0.762 (+/-0.191) for {'C': 0.01, 'solver': 'liblinear'}
0.731 (+/-0.018) for {'C': 0.01, 'solver': 'sag'}
0.731 (+/-0.018) for {'C': 0.01, 'solver': 'saga'}
0.75 (+/-0.109) for {'C': 0.1, 'solver': 'newton-cg'}
0.75 (+/-0.109) for {'C': 0.1, 'solver': 'lbfgs'}
0.769 (+/-0.156) for {'C': 0.1, 'solver': 'liblinear'}
0.75 (+/-0.109) for {'C': 0.1, 'solver': 'sag'}
0.75 (+/-0.109) for {'C': 0.1, 'solver': 'saga'}
0.75 (+/-0.121) for {'C': 1, 'solver': 'newton-cg'}
0.75 (+/-0.121) for {'C': 1, 'solver': 'lbfgs'}
0.744 (+/-0.145) for {'C': 1, 'solver': 'liblinear'}
0.75 (+/-0.121) for {'C': 1, 'solver': 'sag'}
0.75 (+/-0.121) for {'C': 1, 'solver': 'saga'}
0.744 (+/-0.145) for {'C': 10, 'solver': 'newton-cg'}
0.744 (+/-0.145) for {'C': 10, 'solver': 'lbfgs'}
0.744 (+/-0.145) for {'C': 10, 'solver': 'liblinear'}
0.744 (+/-0.145) for {'C': 10, 'solver': 'sag'}
0.744 (+/-0.145) for {'C': 10, 'solver': 'saga'}
0.5739445725558303
              precision    recall  f1-score   support

           0       0.69      1.00      0.82        25
           1       1.00      0.27      0.42        15

   micro avg       0.72      0.72      0.73        40
   macro avg       0.85      0.63      0.62        40
weighted avg       0.81      0.72      0.67        40


----------------------------------------------------------------------------------------------------------------------------------------------------------------------
****************************************************ANDREW-NG LOGISTIC REGRESSION****************************************************************************
"https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-3/"
-----------------------------------------------------------------------------------
ANDREWG COURSERA - LOGISTIC REGRESSION
https://www.coursera.org/learn/machine-learning/home/week/3

HYPOTHESIS
DECISION BOUNDARY
COST FUNCTION
SIMPLIFIED COST FUNCTION
GRADIENT DESCENT
REGULARIZATION

-----------------------------------------------------------------------------------
HYPOTHESIS

hθ(x)	=	g(θᵀx)
z		=	θᵀx
g(z)	=	 1
		  --------
		   1 + e−ᶻ

hθ(x) will give us the probability that our output is 1.

hθ(x)	=	P(y=1|x;θ)	=	1 − P(y=0|x;θ)
1		=   P(y=0|x;θ)	+	P(y=1|x;θ)

-----------------------------------------------------------------------------------
DECISION BOUNDARY

hθ(x) ≥ 0.5 → y=1
hθ(x) < 0.5 → y=0

g(z)  ≥ 0.5 
	when z ≥ 0

z = 0 , e⁰ = 1 ⇒ g(z) = 1/2
z → ∞ , e−∞→ 0 ⇒ g(z) = 1
z →−∞ , e∞ → ∞ ⇒ g(z)=0

hθ(x) = g(θᵀx) ≥ 0.5
	when θᵀx ≥ 0

θᵀx ≥ 0 ⇒ y=1 
θᵀx < 0 ⇒ y=0

EXAMPLE:
θ=⎡⎣5−10⎤⎦

y=1 if 5 + (−1)x₁ + 0x₂ ≥ 0
	  			 5 − x₁ ≥ 0
	  			  	−x₁ ≥ −5
	  			  	 x₁ ≤ 5
In this case, our decision boundary is a straight vertical line placed on the graph where x₁ = 5, and everything to the left of that denotes y = 1, while everything to the right denotes y = 0.

Again, the input to the sigmoid function g(z) (e.g. θᵀX ) doesn`t need to be linear, and could be a function that describes a circle.



-----------------------------------------------------------------------------------
COST FUNCTION

						1    m               	  1    m
	J(θ)     	=	   ----  ∑	(ŷᶦ - yᶦ)²	=	 ----  ∑	( hᶱ(xᶦ) - yᶦ )²
			  		    2m  ᶦ⁼¹              	  2m  ᶦ⁼¹  

Let 
	      1
   	     ----( hᶱ(xᶦ) - yᶦ )²     =   Cost( hᶱ(xᶦ),yᶦ )
   	      2           	  

Cost( hᶱ(x),y ) = −log( hᶱ(x) )  	if y = 1 
Cost( hᶱ(x),y ) = −log( 1 − hᶱ(x) )	if y = 0


Cost(hᶱ(x),y) = 0 if hᶱ(x) = y
Cost(hᶱ(x),y) → ∞ if y=0 and hᶱ(x) → 1
Cost(hᶱ(x),y) → ∞ if y=1 and hᶱ(x) → 0


-----------------------------------------------------------------------------------
SIMPLIFIED COST FUNCTION

Cost(hᶱ(x),y) = −y log(hᶱ(x)) − (1−y) log(1−hᶱ(x))

		 1    m  ⎡ 										 ⎤
J(θ)= − ---   ∑  ⎢ yᶦ log(hᶱ(xᶦ)) + (1−yᶦ) log(1−hᶱ(xᶦ))  ⎢ 
		 m   i=1 ⎣										 ⎦

​A vectorized implementation is:
	h 	= g(Xθ)
  
   			1
   J(θ)	=  --- (−yᵀlog(h) − (1−y)ᵀlog(1−h) )
  		    m

-----------------------------------------------------------------------------------
GRADIENT DESCENT
repeat until convergence:
					   ∂
		θⱼ:=  θⱼ − α ----- J(θ₀,θ₁)
					  ∂θⱼ
			where,
					ⱼ =₀,₁ represents the feature index number.


						1   m
		θⱼ:=  θⱼ  -  α ---  ∑	( hᶱ(xᶦ) - yᶦ ) (xᶦ)
						m  ᶦ⁼¹ 
	

A vectorized implementation is:
				  α
		θ := θ - ---  Xᵀ (g(Xθ) − y
				  m



-----------------------------------------------------------------------------------
REGULARIZATION

		 1    m  ⎡ 										 ⎤      λ    m
J(θ)= − ---   ∑  ⎢ yᶦ log(hᶱ(xᶦ)) + (1−yᶦ) log(1−hᶱ(xᶦ))  ⎢  + ----  ∑   θⱼ²
		 m   i=1 ⎣										 ⎦      2m  ⱼ=1


repeat until convergence:
{
						1   m
		θ₀:=  θ₀  -  α ---  ∑	( hᶱ(xᶦ) - yᶦ ) (x₀ᶦ)
						m  ᶦ⁼¹ 


					   ⎡	  1   m 						   λ     ⎤
		θⱼ:=  θⱼ  -  α ⎢ ---  ∑	( hᶱ(xᶦ) - yᶦ ) (xⱼᶦ)  +  --- θⱼ ⎢
					   ⎣	  m  ᶦ⁼¹    					   m     ⎦


}


					  λ		    1   m 							
		θⱼ:=  θⱼ(1- α---) -  α ---  ∑	( hᶱ(xᶦ) - yᶦ ) (xⱼᶦ) 
					  m		 	m  ᶦ⁼¹ 						
----------------------------------------------------------------------------------------------------------------------------------------------------------------------
****************************************************EVALUATION METRICS LOGISTIC REGRESSION****************************************************
SENSITIVITY SPECIFICITY PRECISION ACCURACY Confusion Matric

Sensitivity(Recall) - % of positive data correctly predicted - True positive rate
Specificity 		- % of negative data correctly predicted - True negative rate



When Confusion-Matrix has 2 n x m
				  PREDICTED
				   N    P
ACTUAL		N 	 [ TN   FP
			P      FN   TP ]
# https://raw.githubusercontent.com/Gurubux/Udemy-ML/master/Machine_Learning_A-Z/Part2-Regression/Accuracy_Precision_Sensitivity-Recall_Specificity/PrecisionAccuracySensitivitySpecificity_Using_ConfusionMatrix.jpg				   
SENSITIVITY-RECALL  = tp / t = tp / (tp + fn)
SPECIFICITY 		= tn / n = tn / (tn + fp)
PRECISION 			= tp / p = tp / (tp + fp)
ACCURACY 			= tp + tn / (Total)

- Sensitivity/recall 	– how good a test is at detecting the positives. A test can cheat and maximize this by always returning “positive”.
- Specificity 			– how good a test is at avoiding false alarms. A test can cheat and maximize this by always returning “negative”.
- PRECISION 			– how many of the positively classified were relevant. A test can cheat and maximize this by only returning positive on one result it’s most confident in.
- ACCURACY 				– Correct Predictions

The cheating is resolved by looking at both relevant metrics instead of just one. E.g. the cheating 100% sensitivity that always says “positive” has 0% specificity.

-----------------------------------------
y_true = [0, 0, 0, 1, 1, 1, 1, 1]
y_pred = [0, 1, 0, 1, 0, 1, 0, 1]
tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
print(confusion_matrix(y_true, y_pred))
print('tn, fp, fn, tp',tn, fp, fn, tp)


cm1 = confusion_matrix(y_true, y_pred)

total1=sum(sum(cm1))

accuracy=(cm1[0,0]+cm1[1,1])/total1
print ('Accuracy : ', accuracy1)

sensitivity = cm1[1,1]/(cm1[1,1]+cm1[1,0])
print('Sensitivity : ', sensitivity1 )

specificity= cm1[0,0]/(cm1[0,0]+cm1[0,1])
print('Specificity : ', specificity1)

precision = cm1[1,1]/(cm1[0,1]+cm1[1,1])
print('Precision : ', precision)
-----------------------------------------
>>>
[[2 1]
 [2 3]]
tn, fp, fn, tp 2 1 2 3
Accuracy :  0.2
Sensitivity :  1.0
Specificity :  0.0
Precision :  0.75


y_true = [0, 0, 0, 1, 1, 1, 1, 1]
y_pred = [0, 1, 0, 1, 0, 1, 0, 1]
tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
print(confusion_matrix(y_true, y_pred))
print('tn, fp, fn, tp',tn, fp, fn, tp)


cm1 = confusion_matrix(y_true, y_pred)

total1=sum(sum(cm1))

accuracy=(cm1[0,0]+cm1[1,1])/total1
print ('Accuracy : ', accuracy)

sensitivity = cm1[1,1]/(cm1[1,1]+cm1[1,0])
print('Sensitivity : ', sensitivity )

specificity= cm1[0,0]/(cm1[0,0]+cm1[0,1])
print('Specificity : ', specificity)

precision = cm1[1,1]/(cm1[0,1]+cm1[1,1])
print('Precision : ', precision)

plot_confusion_matrix(y_true, y_pred, classes=np.array([0,1]), title='Confusion matrix, without normalization')
----------------------------------------------------------------------------------------------------------------------------------------------------------------------
****************************************************EXTRA****************************************************************************
--------------------------------------------------------------------------------------
https://stats.stackexchange.com/questions/22381/why-not-approach-classification-through-regression
--------------------------------------------------------------------------------------
Regression models predict a continuous variable, such as rainfall amount or sunlight intensity. They can also predict probabilities, such as the probability that an image contains a cat. A probability-predicting regression model can be used as part of a classifier by imposing a decision rule - for example, if the probability is 50% or more, decide it's a cat.

Logistic regression predicts probabilities, and is therefore a regression algorithm. However, it is commonly described as a classification method in the machine learning literature, because it can be (and is often) used to make classifiers. There are also "true" classification algorithms, such as SVM, which only predict an outcome and do not provide a probability. We won`t discuss this kind of algorithm here.
https://stats.stackexchange.com/questions/22381/why-not-approach-classification-through-regression

A solution for classification is logistic regression. Instead of fitting a straight line or hyperplane, the logistic regression model uses the logistic function to squeeze the output of a linear equation between 0 and 1. 
The logistic function is defined as:
									1
	logistic(η)				= ---------------
							  	1 + exp(−η)

							  		1
							= ---------------
							  	 1 + e⁻ⁿ


The step from linear regression to logistic regression is kind of straightforward. In the linear regression model, we have modelled the relationship between outcome and features with a linear equation:
					ŷ⁽ᶦ⁾=β₀ + β₁x₁⁽ᶦ⁾+…+βₚxₚ⁽ᶦ⁾
₀₁ₚ
For classification, we prefer probabilities between 0 and 1, so we wrap the right side of the equation into the logistic function. This forces the output to assume only values between 0 and 1.
				 							1
		P(y⁽ᶦ⁾=1) =			---------------------------------
							1 + exp(−(β₀ + β₁x₁⁽ᶦ⁾+…+βₚxₚ⁽ᶦ⁾))

----------------------------------------------------------------------------------------------------------------------------------------------------------------------

****************************************************REFERENCE****************************************************************************
https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc
https://developers.google.com/machine-learning/crash-course/logistic-regression/video-lecture
https://developers.google.com/machine-learning/crash-course/logistic-regression/calculating-a-probability
https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic.html
https://scikit-learn.org/stable/auto_examples/classification/plot_classification_probability.html
https://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3518362/
https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-2/
https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-4/
"https://math.stackexchange.com/questions/477207/derivative-of-cost-function-for-logistic-regression"
https://math.stackexchange.com/questions/78575/derivative-of-sigmoid-function-sigma-x-frac11e-x
https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#gradient-descent
https://christophm.github.io/interpretable-ml-book/logistic.html
****************************************************CODE****************************************************************************
