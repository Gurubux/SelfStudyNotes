LOGISTIC REGRESSION
https://github.com/Gurubux/SelfStudyNotes/blob/master/Logistic_Regression/Logistic_Regression.ipynb
https://github.com/Gurubux/SelfStudyNotes/blob/master/Logistic_Regression/Logistic_Regression_Scratch.ipynb
---------------------------------------------------------------------------------
Supervised Machine Learning: Classification
https://towardsdatascience.com/supervised-machine-learning-classification-5e685fe18a6d
---------------------------------------------------------------------------------
SELF NOTES

X = X1, x2,....
y = [0,1,1,0,0...] 
y_pred = h(θᵀx)
	let, 
		θᵀx = z
	h(z) = sigmoid(z) = 1 / 1 + e−ᶻ

y_pred = h(θᵀx) =  1 / 1 + e−ᶻ

\TO FIND θ?
1. Error 		= (-y * np.log(y_pred)) - ((1-y)*np.log(1-y_pred))

2. Cost_Funtion = 1/m * sum(error)

3. initial θ 	= [0,0,0,....]

4. for (num of iterations):
		cost_hitory = = Cost_Funtion(θ,X,y)

		y_pred   =  1 / 1 + e −θᵀx
		gradient = X.T @ (y_pred - y)  - "DERIVATIVE of Cost_function"
		θ 	     = θ - ( α * gradient )
		
	Return ( θ , cost_hitory)

\Prediction = 		 θᵀx  >  0  = 1
		OR 
		   = sigmoid(θᵀx) > 0.5 = 1

\Decision boundary :
		θᵀx = θ₀ + θ₁ x1 + θ₂ x2 + ...
		  0 = θ₀ + θ₁ x1 + θ₂ x2 + ...
		 x2 = (θ₀ + θ₁ x1 + ...) / θ₂

So Plot, x1, x2 scatter
Then for the line 
	Get x values as min(x1) and max(x1)
	Get y values i.e x2 values for  min(x1) and max(x1)
			i.e x2_1 = (θ₀ + θ₁ min(x1) ) / θ₂
				x2_2 = (θ₀ + θ₁ max(x1) ) / θ₂
	And plot a line joining (min(x1),x2_1)	and (max(x1),x2_2)	
	
---------------------------------------------------------------------------------
INTRO TO LOGISTIC REGRESSION
1. What is  logistic regression?
2. What kind of problems solved?
3. Which situations we should use Logistic regression?

1. What is  logistic regression?
	Logistic regression is a statistical and machine learning technique for classifying records of a dataset, based on the values of the input fields.
	Let’s say we have a telecommunication dataset that we’d would like to analyze, in order	 to understand which customers might leave us next month.	 This is historical customer data where each row represents one customer.
	You’ll use the dataset to build a model based on historical records and use it to predict the future churn within the customer group.
	INDEPENDENT VARIABLE : 
		In logistic regression, independent variables should be continuous; if categorical, they should be dummy or indicator-coded. This means we have to transform them to some continuous value.
	DEPENDENT VARIABLE : 
		logistic regression can be used for both binary classification and multiclass classification

2. What kind of problems solved?
	a. To predict the "probability" of a person having a heart attack within a specified time period, based on our knowledge of the person's age, sex, and body mass index.
	b. To predict the chance of mortality in an injured patient, or to "predict" whether a patient has a given disease, such as diabetes, based on observed characteristics of that patient, such as weight, height, blood pressure, and results of various blood tests, and so on.
	c. In a marketing context, we can use it to predict the "likelihood" of a customer purchasing a product or halting a subscription, as we’ve done in our churn example.
	d. To predict the "probability" of failure of a given process, system, or product.
	e. To predict the "likelihood" of a homeowner defaulting on a mortgage.

	\Notice that in all of these examples, not only do we predict the class of each case, we also measure the probability of a case belonging to a specific class.

3. Which situations we should use Logistic regression?
	Four situations in which Logistic regression is a good candidate:
		a. When the "target field in your data is categorical", or specifically, is binary, such as 0/1, yes/no, churn or no churn, positive/negative, and so on.
		
		b. When you "need the probability" of your prediction, for example, if you want to know what the probability is, of a customer buying a product. Logistic regression returns a probability score between 0 and 1 for a given sample of data. In fact, logistic regressing predicts the probability of that sample, and we map the cases to a discrete class based on that probability.
		
		c. If your data is "linearly separable".
				The decision boundary of logistic regression is a line or a plane or a hyper-plane. A classifier will classify all the points on one side of the decision boundary as belonging to one class and all those on the other side as belonging to the other class. For example, if we have just two features (and are not applying any polynomial processing), we can obtain an inequality like θ_0+ θ_1 x_1+ θ_2 x_2 > 0, which is a half-plane, easily plottable.
				Please note that in using logistic regression, we can also achieve a complex decision boundary using polynomial processing as well, which is out of scope here.

		d. When you need to "understand the IMPACT" of a feature.
				You can select the best features based on the statistical significance of the logistic regression model coefficients or parameters.
				That is, after finding the optimum parameters, a feature x with the weight θ_1 close to 0, has a smaller effect on the prediction, than features with large absolute values of θ_1. Indeed, it allows us to understand the impact an independent variable has on the dependent variable while controlling other independent variables.

					ŷ    = 		P(y = 1|X)
				P(y=0|X) = 1 -  P(y = 1|X)

----------------------------------------------------------------------------------------------------------------------------------------------------------------------
****************************************************ASSUMPTIONS LOGISTIC REGRESSION****************************************************

L M E A N H
  M        
1. ASSUMPTION OF APPROPRIATE OUTCOME STRUCTURE 
	To begin, one of the main assumptions of logistic regression is the appropriate structure of the outcome variable. Binary logistic regression requires the dependent variable to be binary and ordinal logistic regression requires the dependent variable to be ordinal.

2. ASSUMPTION OF OBSERVATION INDEPENDENCE 
	Logistic regression requires the observations to be independent of each other. In other words, the observations should not come from repeated measurements or matched data. The observations should be independent. No repeated measurements.

3. ASSUMPTION OF THE ABSENCE OF MULTICOLLINEARITY ✔
	Logistic regression requires there to be little or no multicollinearity among the independent variables. This means that the independent variables should not be too highly correlated with each other.
		pearsonr(X1, X2)
 		spearmanr(X1, X2) 
 		chi2_contingency()
 		Variance_inflation_factor()

4. ASSUMPTION OF LINEARITY OF INDEPENDENT VARIABLES AND LOG ODDS ✔
	Logistic regression assumes linearity of independent variables and log odds. Although this analysis does not require the dependent and independent variables to be related linearly, it requires that the independent variables are linearly related to the log odds.
		
5. ASSUMPTION OF A LARGE SAMPLE SIZE 
	Finally, logistic regression typically requires a large sample size. A general guideline is that you need at minimum of 10 cases with the least frequent outcome for each independent variable in your model. For example, if you have 5 independent variables and the expected probability of your least frequent outcome is .10, then you would need a minimum sample size of 500 (10*5 / .10).

6. No Noise / outliers ✔
7. Rescale Inputs
8. Can Fail to Converge










"https://www.lexjansen.com/wuss/2018/130_Final_Paper_PDF.pdf"

----------------------------------------------------------------------------------------------------------------------------------------------------------------------
****************************************************ANALYTICS TESTS****************************************************

pandas.crosstab
"https://towardsdatascience.com/doing-eda-on-a-classification-project-pandas-crosstab-will-change-your-life-c61c1cb2c20b"

3. ASSUMPTION OF THE ABSENCE OF MULTICOLLINEARITY
	pearsonr(X1, X2)
		pearsonr(data['Exam 1'], data['Exam 2'])
		>>> (-0.023664586253294567, 0.8152152827846564)

	spearmanr(X1, X2) 
		spearmanr(data['Exam 1'], data['Exam 2'])
		>>> SpearmanrResult(correlation=-0.0372997299729973, pvalue=0.7125481177278083)

	chi2_contingency()
		from scipy.stats import chi2_contingency
		table = pd.crosstab(data['Exam 1'], data['Exam 2'])
		chi2, p, dof, expected = chi2_contingency(table.values)
		print('Chi-square Statistic %0.3f p_value %0.3f' % (chi2, p))
		#>>>Chi-square Statistic 9900.000 p_value 0.239

	Variance_inflation_factor()
		vif = pd.DataFrame()
		vif["VIF Factor"] = [variance_inflation_factor(data.iloc[:,1:].values, i) for i in range(data.iloc[:,1:].shape[1])]
		vif["features"] = data.iloc[:,1:].columns
			VIF Factor	features
			0	7.676457	Exam 1
			1	7.151108	Exam 2
			2	3.762641	Admitted

4. Assumption Of Continuous Ivs Being Linearly Related To The Log Odds
	Logistic regression DOES NOT require the continuous IV(s) to be linearly related to the DV. 
	BUT 
	It DOES require the continuous IV(s) be linearly related to the "LOG ODDS" of the IV though. 
	==> Look for an S-shaped curve
		sns.regplot(x= 'Exam 1', y= 'Admitted', data= data, logistic= True).set_title("Exam 1 Log Odds Linear Plot")
		sns.regplot(x= 'Exam 2', y= 'Admitted', data= data, logistic= True).set_title("Exam 2 Log Odds Linear Plot")

6. No Noise / outliers
		sns.boxplot(x= 'Exam 1', data= data, orient= 'v').set_title("Exam 1 Box Plot")
		sns.boxplot(x= 'Exam 2', data= data, orient= 'v').set_title("Exam 2 Box Plot")
		
------------------------------------------------------------------------------------------------------------------------------------------------------
****************************************************EVALUATION METRICS LOGISTIC REGRESSION****************************************************
1.
from sklearn.metrics import accuracy_score
accuracy_score(pred_test,y_test)
OR
LR.score(X_test,y_test)

2. 
from sklearn.metrics import jaccard_score,confusion_matrix,classification_report,log_loss
jaccard_score(y_test, pred_test) #0.8064516129032258
confusion_matrix(y_test, pred_test) 
>>> 	[[ 9  5]
 		 [ 1 25]]
classification_report(y_test, pred_test)
>>>
	              precision    recall  f1-score   support
	
	           0       0.90      0.64      0.75        14
	           1       0.83      0.96      0.89        26
	
	    accuracy                           0.85        40
	   macro avg       0.87      0.80      0.82        40
	weighted avg       0.86      0.85      0.84        40

log_loss(y_test, pred_test) # 5.180916408915378

3.
CONFUSION MATRIX
 "https://github.com/Gurubux/ML-Analysis-Steps/tree/master/Confusion-Matrix_Sensitivity-Recall_Specificity_Precision_Accuracy"
 	from sklearn.metrics import confusion_matrix
 	cm = confusion_matrix(y_test, y_pred, labels=[0, 1])
 	tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
 	SENSITIVITY-RECALL  = tp / t = tp / (tp + fn)
	SPECIFICITY         = tn / n = tn / (tn + fp)
	PRECISION           = tp / p = tp / (tp + fp)
	ACCURACY            = tp + tn / (Total)

4.	
CROSS-VALIDATION
"https://github.com/Gurubux/ML-Analysis-Steps/tree/master/Cross-Validation"

5.
AREA UNDER THE RECEIVER OPERATING CHARACTERISTIC CURVE ( roc_curve , AUC , roc_auc_score )
"https://github.com/Gurubux/ML-Analysis-Steps/tree/master/ROC_AUC"

	from sklearn.metrics import roc_curve,auc,roc_auc_score
	
	# calculate roc_curve(y-true,y_score(probs)) : fpr, tpr
	fpr, tpr, _ = roc_curve(y_test, probs)
	
	# calculate AUC 
	auc = auc(fpr, tpr)
	
	# calculate roc_auc_score(y_true,y_score(prob))
	roc_auc_score = roc_auc_score(y_test, probs)

	plot_roc_curve(fpr, tpr)
6.
PRECISION-RECALL CURVE -  (precision_recall_curve, AUC, average_precision_score, f1_score)
	from sklearn.metrics import precision_recall_curve,f1_score,auc,average_precision_score
	
	# calculate precision-recall curve(y-true,y_score(probs))
	precision, recall, thresholds = precision_recall_curve(y_test, probs)
	
	# calculate precision-recall AUC
	auc = auc(recall, precision)

	# calculate average precision score (y_true,y_score(prob)) = Σ(Rₙ - Rₙ₋₁)/Pₙ 
	# where Pₙ and Rₙ are the precision and recall at the nᵗʰ threshold.
	ap = average_precision_score(y_test, pred_test) 
	
	# calculate F1 score - F1 = 2 * (precision * recall) / (precision + recall)
	f1 = f1_score(y_test, pred_test)
	
	plot_precision_recall_curve(recall,precision)

7. Akaike information criterion (AIC), the Bayes Information criterion (BIC)
	AIC
		[Number of variables*2] - [2*- Log Likelihood]
			"2*- Log Likelihood" is deviance of LR and its similar to residual sum of squares(RSS) of a linear regression. Ordinary least squares minimizes RSS and LR minimizes deviance.
	BIC
		[No of variable*log(No of obs)] - [2*-Log Likelihood]

def log_likelihood(x, y, weights):
    z = np.dot(x, weights)
    ll = np.sum( y*z - np.log(1 + np.exp(z)) )
    return ll
def gradient_ascent(X, h, y):
    return np.dot(X.T, y - h)
def update_weight_mle(weight, learning_rate, gradient):
    return weight + learning_rate * gradient

8. Cross-validation scores - gridSearchCV
from sklearn.model_selection import cross_val_score
scores = cross_val_score(estimator, X, y, cv=5)
scores       


parameters = {'C':[0.01,0.1,1,10],'solver':['newton-cg', 'lbfgs',  'sag', 'saga'],'penalty' : ['l2' ,'none']}
from sklearn.model_selection import GridSearchCV
#LR_1 = LogisticRegression(C=0.1, solver='liblinear').fit(X_train,y_train)
LR_1 = LogisticRegression()
cv = GridSearchCV(LR_1,parameters, cv = 5)
cv.fit(X_train,y_train)
print_results(cv)   



9. Cumulative gain.
Cumulative Gains Curve is Precentage of Sample[0 to 1]  V/S Gain[0 to 1]

"https://stackoverflow.com/questions/42699243/how-to-build-a-lift-chart-a-k-a-gains-chart-in-python"
from scikitplot.metrics import plot_cumulative_gain
plot_cumulative_gain(y_test, estimator.predict_proba(X_test))




10.
Lift chart.
The lift curve is an evaluation curve that assesses the performance of your model. It shows how many times more than average the model reaches targets.
Lift curve is Precentage of Sample[0 to 1]  V/S Lift[1 to Max]
"https://www.neuraldesigner.com/blog/methods-binary-classification"
"https://campus.datacamp.com/courses/foundations-of-predictive-analytics-in-python-part-1/explaining-model-performance-to-business?ex=7"

from scikitplot.metrics import plot_lift_curve
# Plot the lift curve
plot_lift_curve(y_test, estimator.predict_proba(X_test))
plt.show()


11. Gini Coefficient
"https://www.analyticsvidhya.com/blog/2016/02/7-important-model-evaluation-error-metrics/"

Gini = 2*AUC - 1
Gini is nothing but ratio between area between the ROC curve and the diagnol line & the area of the above triangle. 
#Gini above 60% is a good model. For the case in hand we get Gini as 92.7%.

12.
Kolomogorov Smirnov chart
"https://www.datavedas.com/model-evaluation-in-python/"
Train_Data1 = pd.DataFrame(y_train)
Train_Data1['Prob'] = estimator.predict_proba(X_train)[:, 1]
Train_Data1['decile'] = pd.qcut(Train_Data1['Prob'],10,labels=['1','2','3','4','5','6','7','8','9','10'])
Train_Data1.columns = ['Defaulter','Probability','Decile']
Train_Data1['Non-Defaulter'] = 1-Train_Data1['Defaulter']

Train_Data1.head()
Kolomogorov_Smirnov_chart(Train_Data1)
>>>
		Defaulter_Count	Non-Defaulter_Count	max_score	min_score	Total_Cust	Default_Rate	Default %	Non_Default %	ks_stats	max_ks
Decile										
10					 6		0				0.869832	0.847041	6			100.00%			17.65%		0.00%			17.65	
9					 6		0				0.809714	0.772197	6			100.00%			17.65%		0.00%			35.29	
8					 5		1				0.768802	0.754434	6			83.33%			14.71%		3.85%			46.15	
7					 6		0				0.753353	0.709390	6			100.00%			17.65%		0.00%			63.80	
6					 4		2				0.695553	0.661380	6			66.67%			11.76%		7.69%			67.87	
5					 5		1				0.651538	0.608318	6			83.33%			14.71%		3.85%			78.73		*****
4					 2		4				0.597930	0.488699	6			33.33%			5.88%		15.38%			69.23	
3					 0		6				0.482329	0.424284	6			0.00%			0.00%		23.08%			46.15	
2					 0		6				0.423319	0.406785	6			0.00%			0.00%		23.08%			23.08	
1					 0		6				0.373267	0.239635	6			0.00%			0.00%		23.08%			-0.00	




13.
Concordant – Discordant ratio
"https://www.analyticsvidhya.com/blog/2016/02/7-important-model-evaluation-error-metrics/"
Concordant ratio of more than 60% is considered to be a good model. This metric generally is not used when deciding how many customer to target etc. It is primarily used to access the model’s predictive power. For decisions like how many to target are again taken by KS / Lift charts.
(conc - disc)/pairs_tested
(Concordant - Discordant) / Total pairs
 #THE FOLLOWING CODE CALCULATES CONCORDANCE AND DISCORDANCE
"https://incipientanalyst.wordpress.com/2017/08/08/generic-python-code-for-classification-techniques/"
#THE FOLLOWING CODE CALCULATES CONCORDANCE AND DISCORDANCE
concordance_discordance(estimator,X)
>>>
   Admitted    Prob_1
0         0  0.380022
1         0  0.239635
2         0  0.373267
3         1  0.645897
4         1  0.760383
Pairs =  2400
Conc =  2219
Disc =  181
Tied =  0
Concordance =  0.92 %
Discordance =  0.08 %
Tied =  0.0 %
Somers D =  0.85




"https://www.linkedin.com/pulse/logistic-regression-algorithm-step-amit-kumar/"

Divergence:
[(mean when event equal to one – mean when event equal to zero)^2] / [0.5(variance when event equal to one + variance when event equal to zero)]
"http://machinethinker.com/2017/09/11/a-simple-introduction-to-kullback-leibler-divergence-through-python-code/"



----------------------------------------------------------------------------------------------------------------------------------------------------------------------
****************************************************VISUALIZATION****************************************************
Confusion Matrix
	plot_confusion_matrix2(y_true,predictions_, normalize=True)
	plot_confusion_matrix(y_test,y_pred, classes=np.asarray(["0", "1", "2","3"]), normalize=True)

Learning Curves
	plot_learning_curve(estimator, title, X, y, ylim=(0.7, 1.01), cv=cv, n_jobs=4)

Correlation Coefficient
	plot_Correlation_Coefficient_1(corr)
	plot_Correlation_Coefficient_2(corr)


AUC ROC PRECISION RECALL
	plot_roc_curve(fpr, tpr)
	plot_precision_recall_curve(recall,precision)


from scikitplot.metrics import plot_lift_curve
plot_lift_curve(y_test, estimator.predict_proba(X_test))

from scikitplot.metrics import plot_cumulative_gain
plot_cumulative_gain(y_test, estimator.predict_proba(X_test))


----------------------------------------------------------------------------------------------------------------------------------------------------------------------
****************************************************LOGISTIC VS LINEAR REGRESSION****************************************************
Steps:
"https://raw.githubusercontent.com/Gurubux/CognitiveClass-ML/master/Course_MachineLearningWithPython/4-Classification/LogR_steps.PNG"

Choosing θ :
	Gradient Descent

Stopping Iteration :
	Satisfactory Accuracy Score.


LAB: LOGISTIC REGRESSION
While Linear Regression is suited for estimating continuous values (e.g. estimating house price), it is not the best tool for predicting the class of an observed data point. In order to estimate the class of a data point, we need some sort of guidance on what would be the most probable class for that data point. For this, we use Logistic Regression.

The objective of Logistic Regression algorithm, is to find the best parameters θ, for ℎ_θ(𝑥) = 𝜎({θ^TX}), in such a way that the model best predicts the class of each case.



# write your code here
parameters = {'C':[0.01,0.1,1,10],'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}
from sklearn.model_selection import GridSearchCV
#LR_1 = LogisticRegression(C=0.1, solver='liblinear').fit(X_train,y_train)
LR_1 = LogisticRegression()
cv = GridSearchCV(LR_1,parameters, cv = 5)
cv.fit(X_train,y_train)
print_results(cv)

print(log_loss(y_test, cv.best_estimator_.predict_proba(X_test)))
print (classification_report(y_test, cv.best_estimator_.predict(X_test)))
>>>

BEST PARAMS: {'C': 0.1, 'solver': 'liblinear'}

LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',
          tol=0.0001, verbose=0, warm_start=False)
                                params  mean_test_score
7    {'C': 0.1, 'solver': 'liblinear'}          0.76875
2   {'C': 0.01, 'solver': 'liblinear'}          0.76250
10     {'C': 1, 'solver': 'newton-cg'}          0.75000
14          {'C': 1, 'solver': 'saga'}          0.75000
5    {'C': 0.1, 'solver': 'newton-cg'}          0.75000


0.731 (+/-0.018) for {'C': 0.01, 'solver': 'newton-cg'}
0.731 (+/-0.018) for {'C': 0.01, 'solver': 'lbfgs'}
0.762 (+/-0.191) for {'C': 0.01, 'solver': 'liblinear'}
0.731 (+/-0.018) for {'C': 0.01, 'solver': 'sag'}
0.731 (+/-0.018) for {'C': 0.01, 'solver': 'saga'}
0.75 (+/-0.109) for {'C': 0.1, 'solver': 'newton-cg'}
0.75 (+/-0.109) for {'C': 0.1, 'solver': 'lbfgs'}
0.769 (+/-0.156) for {'C': 0.1, 'solver': 'liblinear'}
0.75 (+/-0.109) for {'C': 0.1, 'solver': 'sag'}
0.75 (+/-0.109) for {'C': 0.1, 'solver': 'saga'}
0.75 (+/-0.121) for {'C': 1, 'solver': 'newton-cg'}
0.75 (+/-0.121) for {'C': 1, 'solver': 'lbfgs'}
0.744 (+/-0.145) for {'C': 1, 'solver': 'liblinear'}
0.75 (+/-0.121) for {'C': 1, 'solver': 'sag'}
0.75 (+/-0.121) for {'C': 1, 'solver': 'saga'}
0.744 (+/-0.145) for {'C': 10, 'solver': 'newton-cg'}
0.744 (+/-0.145) for {'C': 10, 'solver': 'lbfgs'}
0.744 (+/-0.145) for {'C': 10, 'solver': 'liblinear'}
0.744 (+/-0.145) for {'C': 10, 'solver': 'sag'}
0.744 (+/-0.145) for {'C': 10, 'solver': 'saga'}
0.5739445725558303
              precision    recall  f1-score   support

           0       0.69      1.00      0.82        25
           1       1.00      0.27      0.42        15

   micro avg       0.72      0.72      0.73        40
   macro avg       0.85      0.63      0.62        40
weighted avg       0.81      0.72      0.67        40





----------------------------------------------------------------------------------------------------------------------------------------------------------------------
****************************************************ANDREW-NG LOGISTIC REGRESSION****************************************************************************
"https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-3/"
-----------------------------------------------------------------------------------
ANDREWG COURSERA - LOGISTIC REGRESSION
https://www.coursera.org/learn/machine-learning/home/week/3

HYPOTHESIS
DECISION BOUNDARY
COST FUNCTION
SIMPLIFIED COST FUNCTION
GRADIENT DESCENT
REGULARIZATION

-----------------------------------------------------------------------------------
HYPOTHESIS

hθ(x)	=	g(θᵀx)
z		=	θᵀx
g(z)	=	 1
		  --------
		   1 + e−ᶻ

hθ(x) will give us the probability that our output is 1.

hθ(x)	=	P(y=1|x;θ)	=	1 − P(y=0|x;θ)
1		=   P(y=0|x;θ)	+	P(y=1|x;θ)

-----------------------------------------------------------------------------------
DECISION BOUNDARY

hθ(x) ≥ 0.5 → y=1
hθ(x) < 0.5 → y=0

g(z)  ≥ 0.5 
	when z ≥ 0

z = 0 , e⁰ = 1 ⇒ g(z) = 1/2
z → ∞ , e−∞→ 0 ⇒ g(z) = 1
z →−∞ , e∞ → ∞ ⇒ g(z)=0

hθ(x) = g(θᵀx) ≥ 0.5
	when θᵀx ≥ 0

θᵀx ≥ 0 ⇒ y=1 
θᵀx < 0 ⇒ y=0

EXAMPLE:
θ=⎡⎣5−10⎤⎦

y=1 if 5 + (−1)x₁ + 0x₂ ≥ 0
	  			 5 − x₁ ≥ 0
	  			  	−x₁ ≥ −5
	  			  	 x₁ ≤ 5
In this case, our decision boundary is a straight vertical line placed on the graph where x₁ = 5, and everything to the left of that denotes y = 1, while everything to the right denotes y = 0.

Again, the input to the sigmoid function g(z) (e.g. θᵀX ) doesn`t need to be linear, and could be a function that describes a circle.



-----------------------------------------------------------------------------------
COST FUNCTION

						1    m               	  1    m
	J(θ)     	=	   ----  ∑	(ŷᶦ - yᶦ)²	=	 ----  ∑	( hᶱ(xᶦ) - yᶦ )²
			  		    2m  ᶦ⁼¹              	  2m  ᶦ⁼¹  

Let 
	      1
   	     ----( hᶱ(xᶦ) - yᶦ )²     =   Cost( hᶱ(xᶦ),yᶦ )
   	      2           	  

Cost( hᶱ(x),y ) = −log( hᶱ(x) )  	if y = 1 
Cost( hᶱ(x),y ) = −log( 1 − hᶱ(x) )	if y = 0


Cost(hᶱ(x),y) = 0 if hᶱ(x) = y
Cost(hᶱ(x),y) → ∞ if y=0 and hᶱ(x) → 1
Cost(hᶱ(x),y) → ∞ if y=1 and hᶱ(x) → 0


-----------------------------------------------------------------------------------
SIMPLIFIED COST FUNCTION

Cost(hᶱ(x),y) = −y log(hᶱ(x)) − (1−y) log(1−hᶱ(x))

		 1    m  ⎡ 										 ⎤
J(θ)= − ---   ∑  ⎢ yᶦ log(hᶱ(xᶦ)) + (1−yᶦ) log(1−hᶱ(xᶦ))  ⎢ 
		 m   i=1 ⎣										 ⎦

​A vectorized implementation is:
	h 	= g(Xθ)
  
   			1
   J(θ)	=  --- (−yᵀlog(h) − (1−y)ᵀlog(1−h) )
  		    m

-----------------------------------------------------------------------------------
GRADIENT DESCENT
repeat until convergence:
					   ∂
		θⱼ:=  θⱼ − α ----- J(θ₀,θ₁)
					  ∂θⱼ
			where,
					ⱼ =₀,₁ represents the feature index number.


						1   m
		θⱼ:=  θⱼ  -  α ---  ∑	( hᶱ(xᶦ) - yᶦ ) (xᶦ)
						m  ᶦ⁼¹ 
	

A vectorized implementation is:
				  α
		θ := θ - ---  Xᵀ (g(Xθ) − y
				  m



-----------------------------------------------------------------------------------
REGULARIZATION

		 1    m  ⎡ 										 ⎤      λ    m
J(θ)= − ---   ∑  ⎢ yᶦ log(hᶱ(xᶦ)) + (1−yᶦ) log(1−hᶱ(xᶦ))  ⎢  + ----  ∑   θⱼ²
		 m   i=1 ⎣										 ⎦      2m  ⱼ=1


repeat until convergence:
{
						1   m
		θ₀:=  θ₀  -  α ---  ∑	( hᶱ(xᶦ) - yᶦ ) (x₀ᶦ)
						m  ᶦ⁼¹ 


					   ⎡	  1   m 						   λ     ⎤
		θⱼ:=  θⱼ  -  α ⎢ ---  ∑	( hᶱ(xᶦ) - yᶦ ) (xⱼᶦ)  +  --- θⱼ ⎢
					   ⎣	  m  ᶦ⁼¹    					   m     ⎦


}


					  λ		    1   m 							
		θⱼ:=  θⱼ(1- α---) -  α ---  ∑	( hᶱ(xᶦ) - yᶦ ) (xⱼᶦ) 
					  m		 	m  ᶦ⁼¹ 						
----------------------------------------------------------------------------------------------------------------------------------------------------------------------
****************************************************ANALYTICS VIDHYA Q/A****************************************************************************
https://www.analyticsvidhya.com/blog/2017/08/skilltest-logistic-regression/










----------------------------------------------------------------------------------------------------------------------------------------------------------------------
****************************************************EXTRA****************************************************************************
--------------------------------------------------------------------------------------
https://stats.stackexchange.com/questions/22381/why-not-approach-classification-through-regression
--------------------------------------------------------------------------------------
Regression models predict a continuous variable, such as rainfall amount or sunlight intensity. They can also predict probabilities, such as the probability that an image contains a cat. A probability-predicting regression model can be used as part of a classifier by imposing a decision rule - for example, if the probability is 50% or more, decide it's a cat.

Logistic regression predicts probabilities, and is therefore a regression algorithm. However, it is commonly described as a classification method in the machine learning literature, because it can be (and is often) used to make classifiers. There are also "true" classification algorithms, such as SVM, which only predict an outcome and do not provide a probability. We won`t discuss this kind of algorithm here.
https://stats.stackexchange.com/questions/22381/why-not-approach-classification-through-regression

A solution for classification is logistic regression. Instead of fitting a straight line or hyperplane, the logistic regression model uses the logistic function to squeeze the output of a linear equation between 0 and 1. 
The logistic function is defined as:
									1
	logistic(η)				= ---------------
							  	1 + exp(−η)

							  		1
							= ---------------
							  	 1 + e⁻ⁿ


The step from linear regression to logistic regression is kind of straightforward. In the linear regression model, we have modelled the relationship between outcome and features with a linear equation:
					ŷ⁽ᶦ⁾=β₀ + β₁x₁⁽ᶦ⁾+…+βₚxₚ⁽ᶦ⁾
₀₁ₚ
For classification, we prefer probabilities between 0 and 1, so we wrap the right side of the equation into the logistic function. This forces the output to assume only values between 0 and 1.
				 							1
		P(y⁽ᶦ⁾=1) =			---------------------------------
							1 + exp(−(β₀ + β₁x₁⁽ᶦ⁾+…+βₚxₚ⁽ᶦ⁾))

----------------------------------------------------------------------------------------------------------------------------------------------------------------------

****************************************************REFERENCE****************************************************************************
https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc
https://developers.google.com/machine-learning/crash-course/logistic-regression/video-lecture
https://developers.google.com/machine-learning/crash-course/logistic-regression/calculating-a-probability
https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic.html
https://scikit-learn.org/stable/auto_examples/classification/plot_classification_probability.html
https://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3518362/
https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-2/
https://www.johnwittenauer.net/machine-learning-exercises-in-python-part-4/
"https://math.stackexchange.com/questions/477207/derivative-of-cost-function-for-logistic-regression"
https://math.stackexchange.com/questions/78575/derivative-of-sigmoid-function-sigma-x-frac11e-x
https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#gradient-descent
https://christophm.github.io/interpretable-ml-book/logistic.html
https://towardsdatascience.com/real-world-implementation-of-logistic-regression-5136cefb8125
****************************************************CODE****************************************************************************
