Analysis Tests
"https://machinelearningmastery.com/statistical-hypothesis-tests-in-python-cheat-sheet/"
-------------------------------------------------------------------------------------------------------------------
LEARNING CURVE ANALYSIS FOR REGRESSION AND CLASSIFICATION
-------------------------------------------------------------------------------------------------------------------
"https://github.com/Gurubux/StatQuest/tree/master/LearningCurve"



-------------------------------------------------------------------------------------------------------------------
REGRESSION - https://github.com/Gurubux/SelfStudyNotes/tree/master/Linear_Regression
-------------------------------------------------------------------------------------------------------------------
THE PEARSON CORRELATION
SPEARMAN`S RANK ORDER CORRELATION
CHI-SQUARE(X²) TEST
VARIANCE INFLATION FACTOR-(VIF)
OUTLIERS, LEVERAGE, INFLUENCE
COOK`S DISTANCE
BREUSH-PAGAN TEST:
HET_GOLDFELDQUANDT
T-Test
ANOVA




-------------------------------------------------------------------------------------------------------------------
CLASSIFICATION - https://github.com/Gurubux/SelfStudyNotes/tree/master/Logistic_Regression
-------------------------------------------------------------------------------------------------------------------

Multicollinearity
 pearsonr(X1, X2)
 spearmanr(X1, X2) 
 chi2_contingency
 Variance_inflation_factor

Chisquare()

Regplot() : Linearity Of Ivs & Log Odds

Boxplot() : outliers 
"https://pythonfordatascience.org/logistic-regression-python/"


-------------------------------------------------------------
AREA UNDER THE CURVE
-------------------------------------------------------------

-------------------------------------------------------------
PROBABILITY DENSITY FUNCTION - CONTINOUS VARIABLE
-------------------------------------------------------------


-------------------------------------
PROBABILITY IN NORMAL DENSITY CURVES
-------------------------------------

STANDARD NORMAL DISTRIBUTION Table - https://www.mathsisfun.com/data/standard-normal-distribution-table.html
					    ______________  
					   /    x - μ	 /
P(X<K)	=		P(    / z < ______  /  )
					 /	      σ	   / 
					/_____________/

    					______________  
					   /    73 - 70	 /
P(X<73)	=		P(    / z < ______  /  )
					 /	      6	   / 
					/_____________/


		=		P(   z < −0.5   )
p-value		≈       0.6915    (from table)

​
--------------------------------------------------------------------------------------------------
T-test
Draw Confidence level for Normal Curve, Z-Score CDF PDF PPF
draw_z_score(x, x<z0, 0, 1, 'z<-0.75')
sample_means = normal_curve_confidence_shade(population_ages,minnesota_ages,100)
Central-Limit-Theorem
"https://github.com/Gurubux/Data-Lit/blob/master/2-StatisticsAndProbability/2.6-ReadingAssignment_HypothesisTesting/Hypothesis_Testing_Demo_DataLit_Week_2.ipynb"
"https://github.com/Gurubux/ML-Analysis-Steps/blob/master/InferentialStatistics/HypothesisTestingAndTheT-Test/HypothesisTestingAndTheT-Test.txt"
"https://github.com/Gurubux/ML-Analysis-Steps/blob/master/InferentialStatistics/HypothesisTestingAndTheT-Test/HypothesisTestingAndTheT_Test_ConfidenceInterval.ipynb"
"https://github.com/Gurubux/Data-Lit/tree/master/2-StatisticsAndProbability/2.4-CentralLimitTheorem"
"https://github.com/Gurubux/Data-Lit/blob/master/2-StatisticsAndProbability/2.6-ReadingAssignment_HypothesisTesting/ReadingAssignment_HypothesisTesting.txt"
----------------------------------------------------------------------------------------------------------------------
@PARAMETRIC METHODS
-Exploring correlation between variables
	- parametric methods in pandas and scipy
 THE PEARSON CORRELATION
			sb.pairplot(cars)
			pd.plotting.scatter_matrix(mouse); 

			import scipy
			from scipy.stats.stats import pearsonr
			pearsonr_coefficient, p_value = pearsonr(mpg, hp)
			print(`mpg, hp PearsonR Correlation Coefficient %0.3f` % (pearsonr_coefficient))
			print(f`mpg, hp p_value {p_value:.7f}`)
			#calculate the Pearson correlation coefficient
			corr = X.corr()
			#visualize the Pearson correlation coefficient
			sb.heatmap(corr,xticklabels=corr.columns.values, yticklabels=corr.columns.values)

		Pearson Correlation Assumptions
		- Data is normally distributed
		- You have continous, numeric values
		- Your variables are linearly related
COVARIANCE- CORRELATION Coefficient - "https://github.com/Gurubux/ML-Analysis-Steps/tree/master/Correlation_Coefficient"
----------------------------------------------------------------------------------------------------------------------
@NON PARAMETRIC METHODS
	- Used For Categorical, Non Linearly related, Non Normally distributed variables
	1. SPEARMAN`S RANK  CORRELATION
	2. CHI-SQUARE(X²) TEST
-----------------------------------------------------------
1. SPEARMAN`S RANK ORDER CORRELATION-
-----------------------------------------------------------
		__________________
	   /      6Σᵢdᵢ²	 /
ρ =   / 1 - ----------- /
	 /	     n(n²–1)   / 
	/_________________/		   
	Where,
		n is the number of data points of the two variables 
		dᵢ is the difference in the ranks of the iᵗʰ element of each random variable considered. dₓ - dᵧ
		______________________________________________________________________
	   /      6[Σᵢdᵢ² + (1/12)[(m₁³−m₁) + (m₂³−m₂) + (m₃³−m₃) +...(mₖ³−mₖ)]	 /
ρ =   / 1 - --------------------------------------------------------------- /
	 /	     						  n(n²–1)   						   / 
	/_____________________________________________________________________/		
	where,
		k is the number of values that are repeated
		mₖ is the number of times the kᵗʰ term is repeated

ρ = +1 to -1
A ρ of +1 indicates a perfect association of ranks
A ρ of zero indicates no association between ranks and
ρ of -1 indicates a perfect negative association of ranks.
The closer ρ is to zero, the weaker the association between the ranks.
"https://www.toppr.com/guides/business-mathematics-and-statistics/correlation-and-regression/rank-correlation/"

Spearman`s rank correlation
#Checking for independence between features
#spearmanr because drat carb are ordinal datas, i.e. numerical but can be converted to categorical
spearmanr_coefficient, p_value =  spearmanr(X1, X2)

1. Spearman`s rank  correlation - Ordinale Data types- Numerical variables that can be categorized -->  1 0 -1
	Spearman`s rank Correlation Assumptions
		a. Ordinal Variables ( Numeric but may be ranked like a categorical variable)
		b. Related Non Linearly
		c. non-normally distributed
			
	from scipy.stats import spearmanr
	spearmanr_coefficient, p_value = spearmanr(cyl, vs)
	print(`Spearman Rank Correlation Coefficient %0.3f` % (spearmanr_coefficient))
	>>> Spearman Rank Correlation Coefficient -0.814  					"STRONG Negative Correlation"

	spearmanr_coefficient, p_value = spearmanr(cyl, am)
	print(`Spearman Rank Correlation Coefficient %0.3f` % (spearmanr_coefficient))
	Spearman Rank Correlation Coefficient -0.522    					"Negative Correlation"

	spearmanr_coefficient, p_value = spearmanr(cyl, gear)
	print(`Spearman Rank Correlation Coefficient %0.3f` % (spearmanr_coefficient))
	Spearman Rank Correlation Coefficient -0.564   						"Negative Correlation"


-----------------------------------------------------------
CHI-SQUARE(X²) TEST - "https://github.com/Gurubux/ML-Analysis-Steps/tree/master/InferentialStatistics/Chi-SquaredTests"
-----------------------------------------------------------
		______________
	   /   (O - E)²	 /
X² =  / Σ   ______  /
	 /	      E	   / 
	/_____________/		   

https://www.mathsisfun.com/data/chi-square-test.html

CHI-SQUARE to P-VALUE
https://www.mathsisfun.com/data/chi-square-calculator.html

CALCULATE DEGREES OF FREEDOM
Multiply (rows − 1) by (columns − 1)

Example: DF = (2 − 1)(2 − 1) = 1×1 = 1

2. Chi-Square tables - Test for independence between variables -  Null hypothesis= Variables are independent.
	p < 0.5 - Reject Null hypothesis and conclude that the variables are correlated
	p > 0.5 - Accept Null hypothesis and conclude that the variables are "INDEPENDENT"
	Chi-Square Correlation Assumptions
		a. Make sure if variables are categorical or Numeric
		b. If numerical, make sure you have binned them( variable has numeric values 0 - 100, bin them in bins of 10 like 0-10, 11-20, ...91-100 )
from scipy.stats import chi2_contingency
	
	table = pd.crosstab(cars[`cyl`], cars[`am`])
	chi2, p, dof, expected = chi2_contingency(table.values)
	print(`Chi-square Statistic %0.3f p_value %0.3f` % (chi2, p))
	>>>Chi-square Statistic 8.741 p_value 0.013

	table = pd.crosstab(cars[`cyl`], cars[`vs`])
	chi2, p, dof, expected = chi2_contingency(table.values)
	print(`Chi-square Statistic %0.3f p_value %0.3f` % (chi2, p))
	>>>Chi-square Statistic 21.340 p_value 0.000

	table = pd.crosstab(cars[`cyl`], cars[`gear`])
	chi2, p, dof, expected = chi2_contingency(table.values)
	print(`Chi-square Statistic %0.3f p_value %0.3f` % (chi2, p))
	>>>Chi-square Statistic 18.036 p_value 0.001

"p < 0.05 so Reject the Null Hypothesis and conclude that variables are correlated and not independent"


-----------------------------------------------------------
Variance Inflation Factor VIF
-----------------------------------------------------------

from statsmodels.stats.outliers_influence import variance_inflation_factor
# For each X, calculate VIF and save in dataframe
# d1 = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', 'y']
vif = pd.DataFrame()
vif["VIF Factor"] = [variance_inflation_factor(d1.values, i) for i in range(d1.shape[1])]
vif["features"] = d1.columns
	VIF Factor	features
0	9.093393	0
1	1.217386	1
2	1.323574	2
3	1.723241	3
4	1.542681	4
5  \59.700276	5
6  \39.374197	6
7  \15.410430	7
8	8.915790	8
9	10.522763	9
10	1.488241	10
11	2.073611	y

----------------------------------------------------------------------------------------------------------------------
COOK`S DISTANCE
----------------------------------------------------------------------------------------------------------------------
influence = est2.get_influence()
#c is the distance and p is p-value
(c, p) = influence.cooks_distance
ax1.stem(np.arange(len(c)), c, markerfmt=",")

----------------------------------------------------------------------------------------------------------------------
LEVERAGE
----------------------------------------------------------------------------------------------------------------------
from statsmodels.graphics.regressionplots import plot_leverage_resid2
plot_leverage_resid2(est2)

----------------------------------------------------------------------------------------------------------------------
INFLUENCE
----------------------------------------------------------------------------------------------------------------------
from statsmodels.stats.outliers_influence import OLSInfluence
test_class = OLSInfluence(results)
test_class.dfbetas[:5,:]
from statsmodels.graphics.regressionplots import influence_plot
influence_plot(est2 , criterion="cooks")

ax1 = sm.graphics.influence_plot(est2, ax=ax1, criterion="cooks")

----------------------------------------------------------------------------------------------------------------------
Heteroskedasticity tests
----------------------------------------------------------------------------------------------------------------------
BREUSH-PAGAN TEST:
name = ['Lagrange multiplier statistic', 'p-value',
        'f-value', 'f p-value']
test = sms.het_breuschpagan(results.resid, results.model.exog)
lzip(name, test)
>>>
[('Lagrange multiplier statistic', 26.14679144370297),
 ('p-value', 0.0035476957582630322),
 ('f-value', 2.7099146719004765),
 ('f p-value', 0.003096456019249622)]

name = ['F statistic', 'p-value']
test = sms.het_goldfeldquandt(est2.resid, est2.model.exog)
>>>[('F statistic', 0.9673325685917769), ('p-value', 0.5949738916923373)]

----------------------------------------------------------------------------------------------------------------------
T-Test - "https://github.com/Gurubux/ML-Analysis-Steps/tree/master/InferentialStatistics/HypothesisTestingAndTheT-Test"
----------------------------------------------------------------------------------------------------------------------
	ONE-SAMPLE T-TEST : stats.ttest_1samp() : T-test for the MEAN OF ONE GROUP of scores.
	TWO-SAMPLE T-TEST stats.ttest_ind(a= minnesota_ages,b= wisconsin_ages, equal_var=False) : T-test for the MEANS OF TWO INDEPENDENT SAMPLES
	PAIRED T-TEST : stats.ttest_rel(a = before, b = after) : T-test on TWO RELATED samples
	TYPE I AND TYPE II ERROR
	Type I ERROR 	: "false positive" or "false hit".
	Type II ERROR 	: "false negative" or "miss".

----------------------------------------------------------------------------------------------------------------------
ANOVA - "https://github.com/Gurubux/ML-Analysis-Steps/tree/master/InferentialStatistics/AnalysisOfVariance(ANOVA)"
----------------------------------------------------------------------------------------------------------------------
	ONE-WAY ANOVA : stats.f_oneway(asian, black, hispanic, other, white)  
			  	  : stats.multicomp.pairwise_tukeyhsd(endog=voter_age,     # Data
                          		   				  groups=voter_race,   # Groups
                          		   				  alpha=0.05)          # Significance level

----------------------------------------------------------------------------------------------------------------------
TRANSFORM DATASET DISTRIBUTIONS
----------------------------------------------------------------------------------------------------------------------
1. Normalization - 			Value of Observation
				  -------------------------------------
					Sum of All Observation in variable

2. Standardization - Rescaling Data so it has a zero mean and unit Variance

- Normalizing and transforming features with MinMaxScalar() and fit_transform()
import sklearn
from sklearn import preprocessing
mpg_matrix = mpg.values.reshape(-1,1)
		>>>
		array([21. , 21. , 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,
		       16.4, 17.3, 15.2, 10.4, 10.4, 14.7, 32.4, 30.4, 33.9, 21.5, 15.5,
		       15.2, 13.3, 19.2, 27.3, 26. , 30.4, 15.8, 19.7, 15. , 21.4])
		to 
		array([[21. ],[21. ],[22.8],[21.4],[18.7],[18.1],[14.3],[24.4],[22.8],[19.2],[17.8],[16.4],[17.3],[15.2],[10.4],[10.4],[14.7],[32.4],[30.4],[33.9],[21.5],[15.5],[15.2],[13.3],[19.2],[27.3],[26. ],[30.4],[15.8],[19.7],[15. ],[21.4]])

scaled = preprocessing.MinMaxScaler()
scaled_mpg = scaled.fit_transform(mpg_matrix)
		>>>
		array([[0.45106383],[0.45106383],[0.52765957],[0.46808511],[0.35319149],[0.32765957],[0.16595745],[0.59574468],[0.52765957],[0.37446809],[0.31489362],[0.25531915],[0.29361702],[0.20425532],[0.        ],[0.        ],[0.18297872],[0.93617021],[0.85106383],[1.        ],[0.47234043],[0.21702128],[0.20425532],[0.12340426],[0.37446809],[0.71914894],[0.66382979],[0.85106383],[0.22978723],[0.39574468],[0.19574468],[0.46808511]])
plt.plot(scaled_mpg)

# Set Y values Range instead of default 0 to 1
mpg_matrix = mpg.values.reshape(-1,1)
scaled = preprocessing.MinMaxScaler(feature_range=(0,10))
scaled_mpg = scaled.fit_transform(mpg_matrix)
plt.plot(scaled_mpg)


-  Using scale() to scale your features
from sklearn.preprocessing import scale
standardized_mpg = scale(mpg, axis=0, with_mean=False, with_std=False) # Y axis is same 
plt.plot(standardized_mpg) 

standardized_mpg = scale(mpg) # Y axis is changed into - and + values
plt.plot(standardized_mpg)




