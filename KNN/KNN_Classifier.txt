K-NEAREST NEIGHBORS
The example focuses on using demographic data, such as region, age, and marital status, to predict usage patterns. The target field, called custcat, has four possible values that correspond to the four customer groups, as follows: Basic Service, E-Service, Plus Service, and Total Service. Our objective is to build a classifier, for example using the rows 0 to 7, to predict the class of row 8.

"How can we find the class of this customer?	"
	Can we find one of the closest cases and assign the same class label to our new customer?	Can we also say that the class of our new customer is most probably group 4 (i.e. total	service) because its nearest neighbor is also of "class 4"?	Yes, we can. In fact, it is the first-nearest neighbor.

"To what extent can we trust our judgment, which is based on the first nearest neighbor?"
	It might be a poor judgment, especially if the first nearest neighbor is a very specific case, or an "outlier", correct? Now, let’s look at our scatter plot again. Rather than choose the first nearest neighbor, what" if we chose the five nearest neighbors", and did a "majority vote" among them to define the class of our new customer? In this case, we’d see that three out of five nearest neighbors tell us to go For "class 3", which is ”Plus service.”
	In this case, the value of K in the k-nearest neighbors algorithm is 5.

K-NEAREST NEIGHBORS. 
	The k-nearest-neighbors algorithm is a classification algorithm that takes a bunch of labelled points and uses them to learn how to label other points. 
	This algorithm classifies cases based on their similarity to other cases. In k-nearest neighbors, data points that are near each other are said to be "neighbors". 
	K-nearest neighbors is based on this paradigm: "“Similar cases with the same class labels are near each other.”" Thus, the distance between two cases is a measure of their dissimilarity. 
	There are different ways to calculate the similarity, or conversely, the distance or dissimilarity of two data points. For example, this can be done using  "Euclidian distance".

K-NN Algorithm working :
	1. Pick a value For K.
	2. Calculate the distance of unknown case from all cases (holdout from each of the cases in the dataset).
	3. Search for the K observations in the training data that are ‘nearest’ to the measurements of the unknown data point.
	4. Predict the response of the unknown data point using the "most popular response" value from the K nearest neighbors.

There are two parts in this algorithm that might be a bit confusing.
	First, "how to select the correct K";
	Second, "how to compute the similarity between cases", for example, among customers?

	"how to compute the similarity between cases"
		Assume that we have two customers, customer 1 and customer 2. And, For a moment, assume that these 2 customers have only one feature, Age. We can easily use a specific type of Minkowski distance to calculate the distance of these 2 customers. It is indeed, the Euclidian distance. Distance of x1 from x2 is root of 34 minus 30 to power of 2, which is 4. What about if we have more than one feature, for example Age and Income? If we have income and age For each customer, we can still use the same formula, but this time, we’re using it in a 2-dimensional space. We can also use the same distance matrix For multi-dimensional vectors. Of course, we have to normalize our feature set to get the accurate dissimilarity measure. There are other dissimilarity measures as well that can be used For this purpose but, as mentioned, it is highly dependent on data type and also the domain that classification is done For it.
					
					dist((x, y), (a, b)) = √(x - a)² + (y - b)²
					Example - age
						dist(age1, age2)	 = √(age1 - age2)²
					Example - age and Income
						dist((age1,Income1), (age2,Income2)  =  √(age1 - age2)²  + (Income1 - Income2)²

	"how to select the correct K" 
		K in k-nearest neighbors, is the number of nearest neighbors to examine. It is supposed to be specified by the user. So, how do we choose the right K? Assume that we want to find the class of the customer noted as question mark on the chart. What happens if we choose a very low value of K, let’s say, k=1? The first nearest point would be Blue, which is class 1. This would be a bad prediction, since more of the points around it are Magenta, or class 4. In fact, since its nearest neighbor is Blue, we can say that we captured the noise in the data, or we chose one of the points that was an anomaly in the data. 

		OVER-FITTING - A low value of K causes a highly complex model as well, which might result in over-fitting of the model. It means the prediction process is not generalized enough to be used for out-of-sample cases. Out-of-sample data is data that is outside of the dataset used to train the model. In other words, it cannot be trusted to be used for prediction of unknown samples. It’s important to remember that over-fitting is bad, as we want a general model that works for any data, not just the data used for training. 
		UNDER-FITTING - Now, on the opposite side of the spectrum, if we choose a very high value of K, such as K=20, then the model becomes overly generalized. 

		So, how we can find the best value for K? The general solution is to reserve a part of your data for testing the accuracy of the model. Once you’ve done so, choose k =1, and then use the training part for modeling, and calculate the accuracy of prediction using all samples in your test set. Repeat this process, increasing the k, and see which k is best for your model. For example, in our case, k=4 will give us the best accuracy.	


Nearest neighbors analysis can also be used to compute values For a "continuous target".
	In this situation, the average or median target value of the nearest neighbors is used to obtain the predicted value for the new case.
	For example, assume that you are predicting the price of a home based on its feature set, such as number of rooms, square footage, the year it was built, and so on. You can easily find the three nearest neighbor houses, of course not only based on distance, but also based on all the attributes, and then predict the price of the house, as the "median" of neighbors.

EVALUATION METRICS
Jaccard index
					|y∩ŷ|			   |y∩ŷ|
	j(y , ŷ)   =   -------   =   ------------------					   ( 0 ....... to ........ 1) higher the better					
					|y∪ŷ|		  |y| + |ŷ| - |y∩ŷ|



F1-score - Confusion Matrix 
		Precision = Tp / (Tp + FP)
		Recall 	  = Tp / (Tp + FN)
		F1-score  = 2 * (Precision * Recall) / (Precision + Recall)   ( 0 ....... to ........ 1) higher the better


Log Loss :   		y log(ŷ) + (1-y) log(1- ŷ)
		 : -1/n  Σ [y log(ŷ) + (1-y) log(1- ŷ)]
		 															 ( 0 ....... to ........ 1)  lower the better






\LAB: KNN and NearestCentroid

from sklearn.neighbors import KNeighborsClassifier
from sklearn.neighbors.nearest_centroid import NearestCentroid

k = 4
#Train Model and Predict  
neigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)
>>>
	KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=None, n_neighbors=4, p=2,
           weights='uniform')

nc = NearestCentroid()
nc.fit(X_train,y_train)
>>>
	NearestCentroid(metric='euclidean', shrink_threshold=None)




yhat = neigh.predict(X_test)
yhat[0:5]
>>>
	array([1, 1, 3, 2, 4])

from sklearn import metrics
print("Train set Accuracy: ", metrics.accuracy_score(y_train, neigh.predict(X_train)))
print("Test set Accuracy: ", metrics.accuracy_score(y_test, yhat))
>>>
	Train set Accuracy:  0.5475
	Test set Accuracy:  0.32

# We can calucalte the accuracy of KNN for different Ks.
Ks = 10
mean_acc = np.zeros((Ks-1))
std_acc = np.zeros((Ks-1))
ConfustionMx = [];
for n in range(1,Ks):
    
    #Train Model and Predict  
    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)
    yhat=neigh.predict(X_test)
    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)
    std_acc[n-1]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0])

mean_acc
>>> array([0.3  , 0.29 , 0.315, 0.32 , 0.315, 0.31 , 0.335, 0.325, 0.34 ])

metricss = ['cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan']
mean_acc_ = np.zeros((len(metricss)))
std_acc_ = np.zeros((len(metricss)))
ConfustionMx = [];
for n in range(len(metricss)):
    
    #Train Model and Predict  
    neigh = NearestCentroid(metric = metricss[n]).fit(X_train,y_train)
    yhat=neigh.predict(X_test)
    mean_acc_[n-1] = metrics.accuracy_score(y_test, yhat)
    std_acc_[n-1]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0])

mean_acc_
>>> array([0.4 , 0.4 , 0.41, 0.4 , 0.35, 0.41])








#Plot model accuracy for Different number of Neighbors
plt.plot(range(1,Ks),mean_acc,'g')
plt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)
plt.legend(('Accuracy ', '+/- 3xstd'))
plt.ylabel('Accuracy ')
plt.xlabel('Number of Nabors (K)')
plt.tight_layout()
plt.show()

\ACCURACY 			= tp + tn / (Total)



print(classification_report(y_test, y_pred))
>>>
	              precision    recall  	  f1-score   support

           1       0.37      	0.51      0.43        51
           2       0.34      	0.34      0.34        44
           3       0.33      	0.33      0.33        54
           4       0.29      	0.18      0.22        51

   micro avg       0.34      	0.34      0.34        200
   macro avg       0.33      	0.34      0.33        200
weighted avg       0.33      	0.34      0.33        200


print( "The best accuracy was with", mean_acc.max(), "with k=", mean_acc.argmax()+1) 
>>>
	The best accuracy was with 0.34 with k= 9


plot_confusion_matrix(y_test,y_pred, classes=np.asarray(["0", "1", "2","3"]), normalize=True)
plot_confusion_matrix(y_test,y_pred, classes=np.asarray(["0", "1", "2","3"]), normalize=False)


------------------------------------------------------------------------------------------------------------------------------------------------------
****************************************************KNN Parameters/ATrributes/Methods****************************************************
\KNeighborsClassifier
1. metric :
	Metrics intended for real-valued vector spaces:
	"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html"
	identifier		class name			args	distance function
	------------------------------------------------------------------
	'euclidean'		EuclideanDistance			sqrt(sum((x - y)^2))
	'manhattan'		ManhattanDistance			sum(|x - y|)
	'chebyshev'		ChebyshevDistance			max(|x - y|)
	'minkowski'		MinkowskiDistance	p		sum(|x - y|^p)^(1/p)
	'wminkowski'	WMinkowskiDistance	p, w	sum(|w * (x - y)|^p)^(1/p)
	'seuclidean'	SEuclideanDistance	V		sqrt(sum((x - y)^2 / V))
	'mahalanobis'	MahalanobisDistance	V or VI	sqrt((x - y)` V^-1 (x - y))

'euclidean','manhattan','chebyshev','minkowski','wminkowski','seuclidean','mahalanobis'
"https://raw.githubusercontent.com/Gurubux/SelfStudyNotes/master/KNN/distance_formuales.gif"
2. n_neighbors:default = 5
3. Weights= ['uniform','distance' ,callable ]
4.algorithm : {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}


\NearestCentroid:
5.metric : ['cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan']
6. shrink_threshold





\RadiusNeighborsClassifier:
7.radius :default = 1.0
	Range of parameter space to use by default for radius_neighbors queries.
	Metric,Weights,algorithm KneighborsClassifier

NearestNeighbors
All from KneighborsClassifier and RadiusNeighborsClassifier



------------------------------------------------------------------------------------------------------------------------------------------------------
****************************************************KNN ASSUMPTIONS****************************************************
k-NN performs much better if all of the data have the same scale
k-NN works well with a small number of input variables (p), but struggles when the number of inputs is very large
k-NN makes no assumptions about the functional form of the problem being solved




------------------------------------------------------------------------------------------------------------------------------------------------------
****************************************************KNN NOTE****************************************************
@REGARDING THE NEAREST NEIGHBORS ALGORITHMS IF IT IS FOUND THAT TWO NEIGHBORS NEIGHBOR K+1 AND K, HAVE IDENTICAL DISTANCES BUT DIFFERENT LABELS, THE RESULTS WILL DEPEND ON THE ORDERING OF THE TRAINING DATA.







------------------------------------------------------------------------------------------------------------------------------------------------------
****************************************************ANALYTICS VIDHYA KNN****************************************************
"https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/"
https://www.analyticsvidhya.com/blog/2017/09/30-questions-test-k-nearest-neighbors-algorithm/



@kNN algorithm does more computation on test time rather than train time.
	The training phase of the algorithm consists only of storing the feature vectors and class labels of the training samples.
	In the testing phase, a test point is classified by assigning the label which are most frequent among the k training samples nearest to that query point – hence higher computation.
	Lazy Algorithm

@It can be used in both classification and regression 

@ASSUMPTIONS
	k-NN performs much better if all of the data have the same scale
	k-NN works well with a small number of input variables (p), but struggles when the number of inputs is very large
	k-NN makes no assumptions about the functional form of the problem being solved

@kNN algorithm can be used for IMPUTING MISSING VALUE of both categorical and continuous variables.
	"https://github.com/Gurubux/SelfStudyNotes/blob/master/KNN/KNN_Regression/KNN_Regressor_Imputing_missing_values.ipynb"


@Manhattan and Euclidean Distance is designed for calculating the distance between real valued i.e CONTINUOUS features.

@Hamming  Distance is designed for calculating the distance CATEGORICAL variable.


Euclidean - sqrt( (1-2)^2 + (3-3)^2) 	   = sqrt(1^2 + 0^2) = 1
Manhattan - sqrt( mod((1-2)) + mod((3-3))) = sqrt(1 + 0) = 1





@When you increase the k the bias will be increases
	large K means simple model, simple model always condider as high bias

@When you decrease the k the variance will increases
	Simple model will be consider as less variance model


@Outliers
	To be more sure of which classifications you make, you can try increasing the value of k.

@In kNN it is very likely to overfit due to the curse of dimensionality. Which of the following option would you consider to handle such problem?
	In such case you can use either dimensionality reduction algorithm or the feature selection algorithm

@kNN is a memory_based approach is that the classifier immediately adapts as we collect new training data.

@The computational complexity for classifying new samples grows linearly with the number of samples in the training dataset in the wors_case scenario.


@In an overfitted model it seems to be performing well on training data, but it is not generalized enough to give the same results on a new data.

@In case of very large value of k, we may include points from other classes into the neighborhood.

@In case of too small value of k the algorithm is very sensitive to noise

A) The classification accuracy is better with larger values of k
B) The decision boundary is smoother with smaller values of k
C) The decision boundary is linear
D) k-NN does not require an explicit training step

Option A: This is not always true. You have to ensure that the value of k is not too high or not too low.
Option B: This statement is not true. The decision boundary can be a bit jagged
Option C: Same as option B
Option D: This statement is true

@You can implement a 2_NN classifier by ensembling 1_NN classifiers

@The boundary becomes smoother with increasing value of K

@We can choose optimal value of k with the help of cross validation

@Euclidean distance treats each feature as equally important

@The value of N is very large, so option A is correct
What would be the time taken by 1-NN if there are N(Very large) observations in test data?
	A) N*D
	B) N*D*2
	C) (N*D)/2
	D) None of these
		Solution: A
			The value of N is very large, so option A is correct


@The training time for any value of k in kNN algorithm is the same.
What would be the relation between the time taken by 1-NN,2-NN,3-NN.
	A) 1-NN >2-NN >3-NN
	B) 1-NN < 2-NN < 3-NN
	C) 1-NN ~ 2-NN ~ 3-NN
	D) None of these
		Solution: C
------------------------------------------------------------------------------------------------------------------------------------------------------
****************************************************REFERENCES KNN****************************************************
"https://scikit-learn.org/stable/modules/neighbors.html"
"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"
"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestCentroid.html"
"https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/"
"https://www.analyticsvidhya.com/blog/2017/09/30-questions-test-k-nearest-neighbors-algorithm/"
SimpliLearn - "https://www.slideshare.net/Simplilearn/knearest-neighbor-classification-algorithm-how-knn-algorithm-works-knn-algorithm-simplilearn/Simplilearn/knearest-neighbor-classification-algorithm-how-knn-algorithm-works-knn-algorithm-simplilearn" - "https://www.youtube.com/watch?v=4HKqjENq9OU"
Edureka - "https://www.youtube.com/watch?v=6kZ-OPLNcgE"