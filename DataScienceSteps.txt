1. IMPORTING LIBRARIES
2. IMPORTING DATA(CSV, TXT ETC.)
3. PREPROCESSING
4. SPLITTING/MERGING/SAMPLING/BOOTSTRAPPING ETC.
5. DATA ANALYSIS STEPS
6. MACHINE LEARNING STEPS
7. EVALUATION STEPS
8. VISUALIZATION
9. STORY TELLING
10. MISCELLANEOUS
---------------------------------
1. IMPORTING LIBRARIES
---------------------------------
import numpy as np
import pandas as pd
import urllib

from sklearn.preprocessing import Imputer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder,OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import scale
from sklearn.linear_model import LinearRegression
import statsmodels.formula.api as sm
from sklearn.preprocessing import PolynomialFeatures
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor

from scipy.stats.stats import pearsonr
from scipy.stats import chi2_contingency
from scipy.stats import spearmanr


import matplotlib.pyplot as plt
import seaborn as sns
---------------------------------
2. IMPORTING DATA(CSV, TXT ETC.)
---------------------------------
data = pd.read_csv('Data.csv')
data = pd.read_csv('examples/brain_size.csv', sep=';', na_values=".")
#importing file from googleDrive into google Colab
from google.colab import drive
drive.mount('/content/drive')
x = '/content/drive/My Drive/dataset-har-PUC-Rio-ugulino.csv' #Remember to put a slash in the start
data = pd.read_csv(x, sep=';', na_values=".")

X = dataset.iloc[:, 1:2].values # Should always be a matrix( and not a vector), (n,m) even if m is 1 
y = dataset.iloc[:, 2].values # Should always be a vector, (n,)

raw_data = urllib.request.urlopen(url)
dataset = np.loadtxt(raw_data, delimiter=",")

import zipfile
zipfile.ZipFile('names.zip').extractall('.')
names2011 = pd.read_csv('names/yob2011.txt',names=['name','sex','number'])

# Download Example file
!wget -O /resources/data/Example1.txt https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/PY0101EN/labs/example1.txt


------------------------------------------------------------------
3. PREPROCESSING - SCALING, CENTERING, STANDARDIZING, NORMALIZING, BINNING, IMPUTING
------------------------------------------------------------------
3.1 MISSING DATA - COUNT NAN VALUES IN DATAFRAME/ REMOVE THE ROW OR INSERT MEAN HAVING NAN
	i. imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)
			#[axis = 0 for Columns, 1 for rows
			#strategy = 'mean' by default
			#missing_values = Value the need to be replaced]
		imputer = imputer.fit(X[:, 1:3])
		X[:, 1:3] = imputer.transform(X[:, 1:3])

		imputer = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')
		imputer = imputer.fit(df[['num-of-doors']])
		df['num-of-doors'] = imputer.transform(df[['num-of-doors']])
	ii. Final_data.fillna(Final_data.mean(),inplace = True)
				DF.fillna(dict) # dict = {0: 0.1, 5: 1.25} # fill column 0 NAs with 0.1 and column 5 NAs with 1.25
				DF.fillna(method='ffill') #will fill-forward any missing values with values from the last non-null element in the column
	iii.counter_nan = dataframe_all.isnull()#Count number of null or nan in each column
					counter_nan.sum()  #display Series of column nam and counter_nan # Count missing values

3.2 CATEGORICAL DATA
	#Encode Text(France/Spain/Germany , Yes/No) into numbers(0/1/2 , 0/1)
	#Yes/No to 0/1 - Encoding the dependent variable/target variable
	i.	labelencoder_y = LabelEncoder()
		y = labelencoder_y.fit_transform(y)
	#Fully Paid/Charged Off to 0/1 #CONVERTING TARGET VARIABLE TO BOOLEAN # Changing/Replacing values in a column
	ii. di = {"Fully Paid":0, "Charged Off":1}   
		Dataset_withBoolTarget= data_with_loanstatus_sliced.replace({"loan_status": di})
		Final_data["emp_length"] = Final_data["emp_length"].replace({`years`:``,`year`:``,` `:``,`<`:``,'\+':``,`n/a`:`0`}, regex = True)
	#France/Spain/Germany to 0/1/2
	iii.labelEncoder_X = LabelEncoder()
		X[:,0] = labelEncoder_X.fit_transform(X[:,0])
		#But, this might lead to our ML algo consider France < Spain < Germany so we will do DUMMY ENCODING(OneHotEncoder) Convert One Column to Multiple Categorical Columns
		#Encoding the Independent Variable
		labelEncoder_X = LabelEncoder()
		X[:,0] = labelEncoder_X.fit_transform(X[:,0])
		oneHotEncoder_X = OneHotEncoder(categorical_features= [0])
		X = onehotencoder.fit_transform(X).toarray()

	iv. Final_data[`grade`] = Final_data[`grade`].map({`A`:7,`B`:6,`C`:5,`D`:4,`E`:3,`F`:2,`G`:1})# convert Categorical text to a number
		haiti.index = haiti.index.map(int) # use map to change type to int	
		df_can.columns = list(map(str, df_can.columns)) # use map let's convert the column names into strings: '1980' to '2013'.
		# map(), 

	v. bins = np.linspace(min(df["horsepower"]), max(df["horsepower"]), 4)
		>>> 	array([ 48.        , 119.33333333, 190.66666667, 262.        ])
		group_names = ['Low', 'Medium', 'High']
		
		df['horsepower-binned'] = pd.cut(df['horsepower'], bins, labels=group_names, include_lowest=True )
		df[['horsepower','horsepower-binned']].head(20)
		>>>		
			horsepower	horsepower-binned
		0	111			Low
		1	111			Low
		2	154			Medium
		3	102			Low
		4	115			Low
		5	110			Low
		#cut()

3.3 REMOVING COLUMNS
	empty_cols = [i for i in range(45,72)]
	dataset = dataset.drop(dataset.columns[empty_cols],axis=1)

	del_col_names = ["delinq_2yrs",  "last_pymnt_d", "chargeoff_within_12_mths","delinq_amnt","emp_title", "term", "emp_title",	"pymnt_plan","purpose","title", "zip_code", "verification_status", "dti","earliest_cr_line", "initial_list_status", "out_prncp", "pymnt_plan", "num_tl_90g_dpd_24m", "num_tl_30dpd", "num_tl_120dpd_2m", "num_accts_ever_120_pd", "delinq_amnt",  "chargeoff_within_12_mths", "total_rec_late_fee", "out_prncp_inv", "issue_d"] #deleting some more columns
	dataset = dataset.drop(labels = del_col_names, axis = 1) # DF_obj.drop([0,2]) ROWS #DF_obj.drop([0,2], axis=1) COLUMNS

	dataset=Dataset_withBoolTarget.dropna(thresh = 340000,axis=1) #340000 is minimum number of non-NA values, Removing all columns having more than 340000 Nan values
	dataset.dropna(subset=['price', 'horsepower'], inplace=True)
	dataset.dropna(axis=1) # Drop any columns with missing values. DEFAULT axis = 0
	# drop(), dropna()

3.4 REMOVING ROWS
	dataset = dataset.dropna(axis=0) # Drop any rows with missing values. DEFAULT axis = 0

	dataset = dataset[dataset["column_name"] > 0] # Remove any rows without following condition
	dataset.dropna(how='all') # drop only the rows from a DataFrame that contain ALL missing values


3.5 CONVERTING TYPE OF COLUMN
	Final_data["emp_length"] = Final_data["emp_length"].apply(lambda x:int(x))
	Final_data[“emp_length”] = Final_data[“emp_length”].apply(lambda x:pd.to_numeric(x, downcast=’integer’))
	missing_df = df[["normalized-losses","stroke","bore","horsepower","peak-rpm"]] #to float64
	missing_df = missing_df.apply(pd.to_numeric)

3.6 SLICING DATA
	series_obj['row 3':'row 7']
	series_obj[2:7]

3.7 REMOVING DUPLICATES
	dataset.duplicated() # The .duplicated() method searches each row in the DataFrame, and returns a True or False value to indicate whether it is a duplicate of another row found earlier in the DataFrame.
	dataset.drop_duplicates() # To drop all duplicate rows, just call the drop_duplicates() method off of the DataFrame.
	dataset.drop_duplicates(['column 3']) # This method will drops all rows that have duplicates in the column you specify.

3.8 SELECTING COLUMNS WITH SPECIFIC DATA TYPES, DTYPES
	df_numeric = df.select_dtypes(include=[np.float64,np.int64])
------------------------------------------------------
4. SPLITTING/MERGING/SAMPLING/BOOTSTRAPPING/SCALING/GROUPING/FILTERING ETC.
------------------------------------------------------
4.1 SPLITTING THE DATASET INTO THE TRAINING SET AND TEST SET
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
	cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)

4.2 FEATURE SCALING
	i.  #Eculidean Distance is used to plot data or compare data points 
		#So Salary is thousands and Age is 2 digit numbers and thus, feature Scaling is important to plot data properly.
		sc_X = StandardScaler()
		X_train = sc_X.fit_transform(X_train)
		X_test = sc_X.transform(X_test)

		#MinMaxScaler
		mpg_matrix = mpg.values.reshape(-1,1)
		scaled = preprocessing.MinMaxScaler(feature_range=(0,10)) # Default is feature_range=(0,1)
		scaled_mpg = scaled.fit_transform(mpg_matrix)
		plt.plot(scaled_mpg)

		#Using scale() to scale your features
		standardized_mpg = scale(mpg, axis=0, with_mean=False, with_std=False)
		plt.plot(standardized_mpg)

	ii. fields = Final_data.columns.values[:-1]
		data_clean = pd.DataFrame(sc_X.fit_transform(Final_data[fields]), columns = fields)

4.3 MERGING/CONCATENATING/JOINING/APPENDING
	i.	data_clean = pd.concat([subset_of_loanstatus_1, subset_of_loanstatus_0]) #ROWS pd.concat([DF_obj_!, DF_obj_2], axis =1) #COLUMNS
	ii.	flights = flights.merge(airlines, how = 'left', on = 'carrier') #COLUMNS
	iii.data_clean['loan_status'] = Final_data['loan_status'] #ADD A COLUMN OF ONE DATAFRAME TO OTHER 
	iv. DF_obj_variable_added = DataFrame.join(DF_obj, series_obj) #ADD A COLUMN
	v. added_datatable = variable_added.append(variable_added, ignore_index=False)# index 0 1 2 3 4 5 0 1 2 3 4  5
		added_datatable = variable_added.append(variable_added, ignore_index=True)# index 0 1 2 3 4 5 6 7 8 9 10 11

4.4 SAMPLING
	i.	subset_of_loanstatus_1 = loanstatus_1.sample(n=5500)
	ii. data_clean = data_clean.sample(frac=1).reset_index(drop=True)

4.5 GROUP BY 
	i. carrier_flights = pd.Series(flights.groupby('name')['name'].count()) # A 500		B 200	C 700	D 150
	ii.cars_groups = cars.groupby(cars['cyl'])
		cars_groups.mean()
---------------------------------
5. DATA ANALYSIS STEPS
---------------------------------
# Subset to the top 5 airlines
top_five_flights = flights[flights['name'].isin(top_five)]
#Sort Series
carrier_flights = carrier_flights.sort_values(ascending=False) #.sort_values(by=[5], ascending=[False]) # 5 is the column index


# Know Relation between data
pearsonr(data['horsepower'], data['price']) # Between -1 and 1 #r2_Score(in %) = pearsonr^2 


#correlation coefficient matrix plot 1 #correlation matrix plot 2
corrmat = dataset.corr()
plot_Correlation_Coefficient_1(corrmat)
plot_Correlation_Coefficient_2(corrmat)



mtcars.mean()                 # Get the mean of each column
mtcars.mean(axis=1)           # Get the mean of each row
mtcars.median()                 # Get the median of each column

#T-Test CDF PPF PDF interval z-Score 1-sample, 2-sample, pair
#https://github.com/Gurubux/ML-Analysis-Steps/blob/master/InferentialStatistics/HypothesisTestingAndTheT-Test/HypothesisTestingAndTheT_Test_ConfidenceInterval.ipynb
#ONE group t-test Sample vs Population
stats.ttest_1samp(a= minnesota_ages,               # Sample data
                 popmean= population_ages.mean())  # Pop mean
stats.t.ppf(q=0.025,  # Quantile to check
            df=99)  # Degrees of freedom
stats.t.ppf(q=0.975,  # Quantile to check
            df=99)  # Degrees of freedom
stats.t.cdf(x= -3.02424,      # T-test statistic
               df= 99) * 2   # Mupltiply by two for two tailed test*
stats.t.interval(0.95,                        # Confidence level
                 df = 99,                     # Degrees of freedom
                 loc = minnesota_ages.mean(), # Sample mean
                 scale= sigma)                # Standard dev estimate
stats.t.interval(alpha = 0.99,                # Confidence level
                 df = 99,                     # Degrees of freedom
                 loc = minnesota_ages.mean(), # Sample mean
                 scale= sigma)                # Standard dev estimate
draw_z_score(x, (z0 < x) & (x < z1),np.mean(sample_means), np.std(sample_means), '-0.75<z<0.75')
sample_means = normal_curve_confidence_shade(population_ages,minnesota_ages,100)

# Two-Sample T-Test
stats.ttest_ind(a= minnesota_ages, 
                b= wisconsin_ages,
                equal_var=False)    # Assume samples have equal variance?

# TWO RELATED  Paired T-test
stats.ttest_rel(a = before, 
                b = after)



# Building Model - Backward Elimination
SL = 0.05
X_opt = X[:, [0, 1, 2, 3, 4, 5]]
X_Modeled = backwardElimination_1(X_opt, SL)

SL = 0.05
X_opt = X[:, [0, 1, 2, 3, 4, 5]]
X_Modeled = backwardElimination_2(X_opt, SL)


#Chi-Squared Test of independence - Correlation
table = pd.crosstab(cyl, am)
from scipy.stats import chi2_contingency
chi2, p, dof, expected = chi2_contingency(table.values)
print('Chi-square Statistic %0.3f p_value %0.3f' % (chi2, p))
>>>Chi-square Statistic 8.741 p_value 0.013


#Spearman`s rank correlation
from scipy.stats import spearmanr
spearmanr_coefficient, p_value = spearmanr(cyl, vs)
print('Spearman Rank Correlation Coefficient %0.3f' % (spearmanr_coefficient))
>>> Spearman Rank Correlation Coefficient -0.814  					STRONG Negative Correlation
---------------------------------
6. MACHINE LEARNING STEPS
---------------------------------
""" REGRESSION """
from sklearn.linear_model import LinearRegression #Simple Linear Regression
LinearRegression().fit(train_x,train_y)

import statsmodels.formula.api as sm #Multiple Linear Regression
sm.OLS(endog = y, exog = X_opt).fit()

from sklearn.preprocessing import PolynomialFeatures #Polynomial Linear Regression
LinearRegression().fit(PolynomialFeatures(degree = 4).fit_transform(X),y) #L_regressor_2.fit(X_poly,y) -> X_poly = PolynomialFeatures(degree = 4).fit_transform(X)

from sklearn.svm import SVR
regressor = SVR(kernel = 'rbf') # ['rbf', 'linear', 'poly']

from sklearn.tree import DecisionTreeRegressor
DecisionTreeRegressor(random_state = 0).fit(X, y) #Decision Tree Regression CART

from sklearn.ensemble import RandomForestRegressor #Random Forest Regression CART
RandomForestRegressor(n_estimators = 10, random_state = 0).fit(X, y)

from scipy.optimize import curve_fit
popt, pcov = curve_fit(sigmoid, xdata, ydata)

from sklearn.pipeline import Pipeline
pipe = Pipeline([
    ('scale', StandardScaler()),
    ('pca', PCA(n_components=5)),
    ('svr', SVR()),
])



---------------------------------
7. EVALUATION STEPS
---------------------------------
 
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
mae = mean_absolute_error(y, pred)
mse = mean_squared_error(y, pred)
r2  = r2_score(test_y_ , test_y) # Between 0 and 1 #r2_Score(in %) = pearsonr^2 


from sklearn.model_selection import learning_curve # learning curve
train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)
plot_learning_curve()
plot_learning_curve_2()


# ROC_AUC - https://github.com/Gurubux/Udemy-ML/tree/master/Machine_Learning_A-Z/Part2-Regression/ROC_AUC
from sklearn.metrics import roc_curve  
from sklearn.metrics import roc_auc_score  
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import f1_score
from sklearn.metrics import average_precision_score
from sklearn.metrics import auc
plotAUC()
plot_roc_curve()


from sklearn.model_selection import cross_val_score # Cross-Validation
cross_val_score(RandomForestClassifier(n_estimators=10)/LogisticRegression()/svm.SVC(), X, y, scoring='accuracy', cv = 10) # [0.67532468 0.76623377 0.67532468 0.66233766 0.79220779 0.74025974 0.76623377 0.77922078 0.72368421 0.77631579]
cross_val_score(RandomForestClassifier(n_estimators=10)/LogisticRegression()/svm.SVC(), X, y, scoring='accuracy', cv = 10).mean() * 100 # 74.48051948051948

from sklearn.model_selection import ShuffleSplit# Cross-Validation for Learning Curve
cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)

from sklearn.metrics import confusion_matrix #Used to calculate Sensitivity, Specificity, Precision and Accuracy
confusion_matrix(y_true, y_pred) # [[2 1] [2 3]] tn, fp, fn, tp 2 1 2 3

 # calculate AUC
from sklearn.metrics import roc_auc_score 
auc = roc_auc_score(testy, probs)

# calculate roc curve
from sklearn.metrics import roc_curve
fpr, tpr, thresholds = roc_curve(testy, probs)

# calculate precision-recall curve
from sklearn.metrics import precision_recall_curve
precision, recall, thresholds = precision_recall_curve(testy, probs)

# calculate F1 score
from sklearn.metrics import f1_score
f1 = f1_score(testy, yhat)

# calculate precision-recall AUC
from sklearn.metrics import auc
auc = auc(recall, precision)

# calculate average precision score
from sklearn.metrics import average_precision_score
ap = average_precision_score(testy, probs)

# calculate accuracy Score - Classification - Naive Bayes - BernoulliNB, GaussianNB, MultinomialNB - ACCURACY  = tp + tn / (Total) 
from sklearn.metrics import accuracy_score
accuracy_score(y_expect, y_pred)


pipe.fit(X_train, y_train)
pipe.score(X_test, y_test)
---------------------------------
8. VISUALIZATION
---------------------------------
8.1 # Histogram of Single/multiple columns using pandas,matplotlib, seaborn, plotly
data = data[['CYLINDERS','ENGINESIZE','CO2EMISSIONS','FUELCONSUMPTION_COMB']]
data.hist()
plt.hist(flights['arr_delay'], color = 'blue', edgecolor = 'black',
         bins = int(180/5))
#seaborn hist=True
sns.distplot(flights['arr_delay'], hist=True, kde=False, 
             bins=int(180/5), color = 'blue',
             hist_kws={'edgecolor':'black'})
#plotly
data = [go.Histogram(x=flights['arr_delay'],
                     xbins=dict(
                            start=min(flights['arr_delay']),
                            end=max(flights['arr_delay']),
                            size=5
                        ),#or nbinsx=36
                     marker = dict(
                         line = dict(
                             color = "black",
                                width = 1
                         )
                     ))]
py.iplot(data, filename='basic histogram')
# More types of histograms 
# https://github.com/Gurubux/Data-Lit/blob/master/2-StatisticsAndProbability/2.5-Homework_Airlines/airlines_least_delay_new.ipynb

8.2 plt.scatter(X_train, y_train, color = 'red') #Plots the points as per given X,Y

8.3 plt.plot(X_train, regressor.predict(X_train), color = 'blue') #Draws a line through the X,Y points

8.4 #Plotting Learning Curve
plot_learning_curve()
plot_learning_curve_2()


8.5 #Plotting Correlation Coefficients
corr = data_clean.corr()
plot_Correlation_Coefficient_1(corr) # https://github.com/Gurubux/ML-Analysis-Steps/tree/master/Correlation_Coefficient
plot_Correlation_Coefficient_2(corr) 


8.6 #Visualizing Confusion matrix 
plot_confusion_matrix() #https://github.com/Gurubux/Udemy-ML/tree/master/Machine_Learning_A-Z/Part2-Regression/Confusion-Matrix
plot_confusion_matrix2() 

8.7 # Descriptive Statistics Plotting
	#plotting Curve on the data spread - matplotlib
	norm_data = pd.DataFrame(np.random.normal(size=100000))
	norm_data.plot(kind="density",figsize=(10,10))
	skewed_data = pd.DataFrame(np.random.exponential(size=100000))
	skewed_data.plot(kind="density",figsize=(10,10),xlim=(-1,5))

	#plotting Curve on the data spread - Seaborn - matplotlib #kde=False
	sns.distplot(flights['arr_delay'], hist=True, kde=False, bins=int(180/5), color = 'blue', hist_kws={'edgecolor':'black'})


 	# Plot black line at mean
 	plt.vlines(norm_data.mean(),ymin=0, ymax=0.4,linewidth=5.0)

 	#Box Plot
 	mtcars.boxplot(column="mpg",return_type='axes',figsize=(8,8))
 	speed1m.plot.box()

	plt.text(x=0.74, y=22.25, s="3rd Quartile")
	plt.text(x=0.8, y=18.75, s="Median")
	plt.text(x=0.75, y=15.5, s="1st Quartile")
	plt.text(x=0.9, y=10, s="Min")
	plt.text(x=0.9, y=33.5, s="Max")
	plt.text(x=0.7, y=19.5, s="IQR", rotation=90, size=25)

8.8 #ROC AUC Curve
plotAUC()
plot_roc_curve()

8.9 #Draw Confidence level for Normal Curve, Z-Score -  CDF PDF PPF
draw_z_score(x, x<z0, 0, 1, 'z<-0.75')
sample_means = normal_curve_confidence_shade(population_ages,minnesota_ages,100)
#https://github.com/Gurubux/ML-Analysis-Steps/blob/master/InferentialStatistics/HypothesisTestingAndTheT-Test/HypothesisTestingAndTheT-Test.txt https://github.com/Gurubux/ML-Analysis-Steps/blob/master/InferentialStatistics/HypothesisTestingAndTheT-Test/HypothesisTestingAndTheT_Test_ConfidenceInterval.ipynb

8.10 #Non-Linear Curve Plotting
#https://github.com/Gurubux/CognitiveClass-ML/blob/master/Course/3-Regression/3.6Non-LinearRegression/CC_3_6_NonLinearRegression.ipynb

8.11 #Bar charts - Bar plot
plt.bar(x, y)
mpg.plot(kind='bar')
mpg.plot(kind='barh')

8.12 # Creating a pie chart
x = [1,2,3,4,0.5]
plt.pie(x)
cars['cyl'].value_counts()
8    14
4    11
6     7
plt.pie(list(cars['cyl'].value_counts()))

8.13 #Defining elements of a plot
#Defining axes, ticks, limitis, xlim, ylim, grids, subplots, xlabel, ylabel , legend, annotate
rcParams['figure.figsize'] = 8, 4
plt.style.use('seaborn-whitegrid')
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (20,6)
plt.style.use('seaborn-whitegrid')

fig = plt.figure()
ax = fig.add_axes([.1, .1, 1, 1]) 

ax.set_xlim([1,9])
ax.set_ylim([0,5])

ax.set_xticks([0,1,2,4,5,6,8,9,10])
ax.set_yticks([0,1,2,3,4,5])

ax.set_xticklabels(cars.car_names, rotation=60, fontsize='medium')

ax.grid()

fig = plt.figure()
    fig, (ax1, ax2) = plt.subplots(1,2)

ax1.plot(x)
ax2.plot(x,y)

ax.plot(x,y)

plt.xlabel('your x-axis label')
plt.ylabel('your y-axis label')

plt.legend(veh_type, loc='best')

ax.annotate('Toyota Corolla', xy=(19,33.9), xytext = (21,35),
           arrowprops=dict(facecolor='black', shrink=0.05))

8.14 #Plot Formatting  plot color, line styles,markers
# https://github.com/Gurubux/LinkedIn-Learn/blob/master/1-MasterPythonForDataScience/1-3-PythonForDataScienceEssentialTraining/Ex_Files_Python_Data_Science_EssT/Exercise%20Files/Ch02/02_03/02_03.ipynb
wide = [0.5, 0.5, 0.5, 0.9, 0.9, 0.9, 0.5, 0.5, 0.5]
color = ['salmon']
plt.bar(x, y, width=wide, color=color, align='center')

color_theme = ['darkgray', 'lightsalmon', 'powderblue']
df.plot(color=color_theme)

color_theme = ['#A9A9A9', '#FFA07A', '#B0E0E6', '#FFE4C4', '#BDB76B']
plt.pie(z, colors = color_theme)

plt.plot(x, y, ls = 'steps', lw=5)
plt.plot(x1,y1, ls='--', lw=10)

plt.plot(x, y, marker = '1', mew=20)
plt.plot(x1,y1, marker = '+', mew=15)

8.15 # Correlation Scatter_matrix pairplot
sb.pairplot(cars)
pd.plotting.scatter_matrix(cars); 

---------------------------------
9. STORY TELLING
---------------------------------


--------------------------
MISCELLANEOUS
----------------------------
# get column and convert it to numpy array
x = dataframe_all.iloc[:,:-1].values

#Series Filtering
counter_without_nan = counter_nan[counter_nan==0]
#Dataframe Filtering
flights = flights[flights['arr_delay'].between(-60, 120)]

# get column names
column_names = dataframe.columns.values[:-1]

 #counts of unique values.
count_unique_values = dataframe['column_name'].value_counts()

#PDF PPF CDF Normal Distribution
vals = stats.norm.ppf([0.001, 0.5, 0.999])
>>>array([-3.09023231,  0.        ,  3.09023231])
np.allclose([0.001, 0.5, 0.999], stats.norm.cdf(vals))

stats.norm.cdf(vals)
>>>array([0.001, 0.5  , 0.999])
stats.norm.pdf(vals)
>>>array([0.00336709, 0.39894228, 0.00336709])
stats.norm.pdf(vals,loc=3, scale=2) #loc = mean scale = std
>>>array([0.00193344, 0.0647588 , 0.19926823])


#GoogleDrive Mount
from google.colab import drive 
drive.mount('/content/gdrive')

#To open Interactive notebooks
https://nbviewer.jupyter.org
# "https://nbviewer.jupyter.org/github/llSourcell/Data_Visualization/blob/master/Plotly%20Whirlwind%20Introduction-Extended.ipynb"

#Saving a plot - Saving a figure
plt.savefig('pie_chart.png')

# Renaming / correcting column names # .columns
cars.columns = ['car_names','mpg','cyl','disp', 'hp', 'drat', 'wt', 'qsec', 'vs', 'am', 'gear', 'carb']
# rename columns
final_df.rename(columns={0: 'Region Name', 1: 'Region Status', 2: 'ISO', 3: 'ANSI_Letter', 4: 'ANSI_Code'
                         , 5: 'USPS', 6: 'USCG', 7: 'GPO', 8: 'AP', 9: 'Other Abbreviations'}, inplace=True)

# setting index / resetting index / making a column index
cars.index = cars.car_names

#categorical data with crosstabs
pd.crosstab(cars['am'], cars['gear'])
gear	3	4	5
am			
0		15	4	0
1		0	8	5

# export to csv, check csv when finished
df_export.to_csv('data/output/YearlyProductSalesTotals.csv', header=True, index=False, encoding='utf-8')

# export to json, check json when finished
df_export.to_json('data/output/YearlyProductSalesTotals.json', orient='records')

# export to excel, check excel file when finished
df_export.to_excel('data/output/YearlyProductSalesTotals.xlsx', header=True, index=False)


SAVE AND LOAD MODELS
import pickle
with open('model.pickle', 'wb') as out:
    pickle.dump(pipe, out)
with open('model.pickle', 'rb') as fp
    print(fp.readlines())

SAVE AND LOAD NUMPY
np.save('example.npy',a)

a1 = np.load('example.npy')

CONVERT SERIES TO DATAFRAME 
to_frame()
df['drive-wheels'].value_counts().to_frame()

DESCRIVE OBJECT TYPE
df.describe(include=['object'])